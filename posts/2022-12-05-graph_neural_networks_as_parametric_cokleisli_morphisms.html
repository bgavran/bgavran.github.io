<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114795894-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114795894-3');
</script>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="author" content="Bruno Gavranovic" />
        <meta name="viewport" content="width=device-width" />
        <meta http-equiv="Cache-Control" content="max-age=86400, must-revalidate" />
        <title>Bruno Gavranović</title>
        <script src="../css/jquery.js"></script>
        <script src="../css/selectfile.js"></script>
        <link rel="stylesheet" type="text/css" title="hakyll_theme" href="../css/theprofessional.css" />
        <link href="https://fonts.googleapis.com/css?family=Titillium+Web" rel="stylesheet">
    </head>
    <body>
        <div class="highbar">&nbsp;</div>
        <div class="container-gallery">
        <div id="content" class="inside">

        <div id="header">
          <div class="box">
            <div id="logo" class="name">
                <h2><pageTitle><a href="../">Bruno Gavranović</a></pageTitle></h2>
            </div>
            <div id="navigation" class="pageslinks">
              <nav class="menuNav">
                <div class="menuItems">
                <a href="../" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Home</a>
                <a href="../archive.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Posts</a>
                <a href="../papers.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Papers</a>
                <a href="../research_philosophy.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Research Philosophy</a>
                <a href="../about.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">About</a>
		<!-- <a href="/contact.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Contact</a> -->
                </div>
              </nav>
            </div>
        </div>
        </div>
            <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>

<div class="info">
    Posted on December  5, 2022
    
</div>

<h1 id="graph-convolutional-neural-networks-as-parametric-cokleisli-morphisms">Graph Convolutional Neural Networks as Parametric CoKleisli morphisms</h1>
<p>This is a short blog post accompanying the latest preprint of Mattia Villani and myself which you can now find <a href="https://arxiv.org/abs/2212.00542">on the ArXiv</a>.</p>
<center>
<img src="../images/abstract_transparent.png" alt="Abstract of the paper" width="600" />
</center>
<p>This paper makes a step forward in substantiating our existing framework described in <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a>. If you’re not familiar with this existing work - it’s a general framework for modeling neural networks in the language of category theory. Given some base category with enough structure, it describes how to construct another category where morphisms are <em>parametric</em>, and <em>bidirectional</em>. Even more specifically - it allows us to describe the setting where the information being sent backwards is the derivative of some chosen loss function.</p>
<p>This is powerful enough to encompass a variety of neural network architectures - recurrent, convolutional, autoregressive, and so on. What the framework doesn’t do is describe the structural essence of all these architectures at the level of category theory.</p>
<p>Our new paper does that, for one specific architecture: Graph Convolutional Neural Networks (GCNNs). We show that they arise as a morphisms for a particular choice of the base category - the CoKleisli category of the product comonad.</p>
<p>It’s a pretty straightforward idea. It’s based on the central observation that a GCNN layer has a different composition rule than just a regular feedforward layer. A simple feedforward layer is often thought of as a map <img width="96" alt="X \mapsto \sigma(XW)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMEAAAAiCAQAAABMm3U7AAAEI0lEQVR4nOVa29mkIAy1BVqwBVugBd/22RZsgRb4tgNbsAVbsAVbmOWiwy1A4tyc2eO3D8vPYMIJyQkzTfPr6Br5aRN2sGZS//479M1yIbc7nDVtM0ePAw/GxctMfRa6ZlX+xKB7GH/CeS6zK4WfOTZ+VCRUwZQZvfrQzTxC/c//22BGNzXeKwevDaYIGMBxqof2E26+85yrZzHjk5oRrtSrMbuSP44O3dYsCzF2+5qMJoKojEH3UJMGrSjMSulp0wRtSaDq93JgLoDJLBxPlpcpbjXUXaV6OGcoE+A69i89MCoxyUiDm4VDc8TXEKAdXSszqB7a1JVizFCgFRCETs2GEiSANTpgwxcRwJTlY3UWzUMJUsD2VVIKZLZaLsUU6WHYyw/GvPNARwQJYyY7h6B5KEAKxD4eE17qRwTKOoPNVHRrHjJ/kTG/hNq5moYsKB7arQ4jWycbHhB5oCRaOOqMei8djPCiqSB8zzCqLaArrM7IwfDxV7ll8nAMiodQ2ZUqmhlAQV/ZAax9u3BbvdYCB44XXmrljdjkiT37xo+LKw5EJQyKhzyhoNvfklJQy/UL8pQ2RwmCpFUJFAr0lm7YzKg2ajHSUBjtYlsiYZ7R28SR8H68hzyZeSSbW9QxwGLUxwxqKxADINzqoFGgYwKX6CwBrnwPhWYJ+368h3HO7++WrIEdOTF6ykJmDkyu9ysbS6GAGRLq75gT/bSCtk1J2Sy9GethG1HgNjrsm/Ni1AFJAVPLdpFww4FKgd2KraISBqCIzeCbsMec6qE/z7/5mU09sehQK6EosObZcnnDFw+DP2C5xDxrIYJWILYfoYDu4e0eBCxIXX7fjLtBE8l5Bsxb7lMO4YaHPgV/AdlYeiZzmPNlrG+gG5rtNAVnPHRlVwaJyzVtPVLvi7oEmDyD2qTi10BPRLKpNSsCSBbasg2YqymoxeIZD4+cHycbRwF2l6qJSEYRYe8U8d8PUCkQwKVujBmImzGjZeqZ9pyHx11pnGyOvlmgva5YKJNog+4US6BRwFFJAMr6sB6qU3DWw3nvDGJrh30c2fE2lvKsBoPvy61ww/bINApmlOnptvZZHdMX09p5D49mMAbfpQReuheqlcxkM5o0ZYQrDY7S5OlZaVXyymXetmDrIx7mvpyBL+pKAG1vlRFhl+fA9pfjLxLwyG0JNNP10Pq3CLJA8wqs+riHAjwDlgLKZSMQIm2kzsPFxuivFG1UR60h8yGMkOzNN2JlSScjL57jYf6eH335bDCQxM3LwQnRY3/LwBFnkdd19wl02Y3GF2I7m9bqfilW4ra8D4xYN74WA0HBvRfDqS+pvhLLRWNtvahdL0D3EgX3KFA/afwdlH9P9wl0xbvgn0R6FfFJ6NvZ5+u0yyO+kPskpgvZ8lZc5eAzuDL9A/GyifGXVQIEAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-5px;" />, where <img width="20" alt="W" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAYCAQAAAAijXdLAAABN0lEQVR4nJWVbRmFIAyFV4EKVqACFahgBStQgQpWsIIVrHAreCcflw0Gcg9/eIZ7ZXKYAFELHGw4yPLVShHPUcCkwICFOwyHc/1bMTjOEN/xCcNyLMZu+IQMUQcuH0LcBeAirBjEaSFOgGcXKO3C4S4HOkJiq60DVFjyUF4EKrg6QD8q95ETgS7FtyquEfiimMjf+pRl0ulz7bVVekBemsfTVQLQNi8QZBqgTmktULJXB0iNkMu6K4e+2IUDy04srGl2MeCrXbKWCljS+B16tUsRBdIb+gCvNNczx0GBcV+KuYzeoQm7cGAszbNmUCxvG4MPlb9VXVYBTtmFA59+U5eV71C3842AN7FL1prik3Yp8uRYqKJDL7HJDtVrpXJ7mARKZZnw7/jDLhQol9X2wynpbtrkcXwBuGOjXTaKpuwAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> is the weight matrix of that layer, and <img width="12" alt="\sigma" height="8" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAQCAQAAACmLXceAAAArElEQVR4nI1SURXDIAyMBSxgAQtYwAIWaiEWsDALtYCFWqiF7ULfa7eQ8Hb5gXBHLgQiC4nyFMFkAkwHvY3YLHKgjqMOEVMbtNdYM+hGhYte733FbveMCPYfukDMRY9eh4H5iuwJ5Lb0v6CMZjVOX8A4YpWLyJ2eISleVG5Drq0EuvjyjXgSFMPkF7KaQYT75dAIbvs9/oR187/bY6ujSgH1mB7AQRgf2W1U8AELTUeGe9QIQgAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-1px;" /> is some activation function. Then two layers are composed using the <a href="https://ncatlab.org/nlab/show/para+construction"><strong>Para</strong></a> construction which defines a map <img width="140" alt="X \mapsto \sigma(\sigma(XW)W&39;)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAARkAAAAiCAQAAADB02VGAAAFNUlEQVR4nN1b3bWkIAy2BVqYFmyBFnzbZ1uYFmjBsx3Ygi1MC7YwLcwi6CAQIJmg4+zn2XP2ekmCSSA/cJvmf0fbDF+lrwnRjPrf0ZDN7URpl0PXPFifzaWvjfbw+fTNq3nSpN2aKXgcpPdeHTHjqmibeV0xdempOgrHO80NST4+jTPcXZuxNB8O/0G7zJyVFkHoj+40k5d5lP5p/7vevH3q951W6LUh9Kf3h9BTdWTHu9FOc1I/D/N+1CN8Pp1+Z/ns3zfAYq3J/6nf3bPSErgZMZCHvX4mwilvVdWnp+pocTGIozJ8oN1MagPGC3ORK4HRNfhL40T7uaekARiNoHDwcKFkMA/Cp35MT9PRlHAwBXKxv+lATgPIpwZ/ZfYpjDQA0gjyP179jMMsHzqXBzHpaTqygSzGPWHSpWKB0erxcciswX8G9iNYGog52ND6H3IYEUTko+gpOhpAk4qVR2zSIZMtPoAQxOd/MyEVJw1Ev6ZT20/HOAzag0m4J6J3bXqKjhRoUrW+Dx003w9SwPz4/O/JAIbW5vNdo/fYeEbGdIgrTsywhKfH68iazl/ZS3CQntttyBcZEnACPv8x8dWQtATsJHpTqNGqJHzP5h5k6Di0pnz0nz0XeIOtR++A1xGUhg569QrApF1Rg/EMufxFZsdH68OWkbPX6sFAEqoVYToDFKg1OoePWwcSWFX16H3gdSQjk7arlNik5dzhEe0IXP7tu+uLkZaETangUi8NisssJnyi8w5hGlMPE6Ht3Ebzf6UNvu+QpuRz6SFgdSSjcVtwWN7vjZgqrveYorylLv+StCRsekfNNmgus/gwLvBZg7vNs2/SzStIPpceBlZHYU7RvWcye/NIF9f5OdblX5KWgDAbUqp3mAbVZYRxmrKMKYq2UB/BttigApVLDwGvo1tgUmc4v2+bK64dYiPW5V+SBkJoMW1QRuJAdRmr+GchK++BJGwCJcHbKJceAk1H+1H7k52pcUeBLVLXkBFr8i9Li2CVYdPTF7Fk/QOml5hnzqyAGVj7FJNz6WNQdfR6O63wAtm+b4s9wVPRjlmXf1lagGXVb0O2MhKPZZf5C5SxuWc0m2c6Lesa6ATliTY5lz4GXUcuDR28MOaacB26Z62aOOWuyb8sLcC4+/xblHGXQA9MS9WR/xjowGyZGVQYLiYP1xKXPgZdR1tOEQYHZ1K8lqFQUZN/WZqHIVgv9sQWnzRRXUaBh/w+JsDPl2IYqlRS6uTQh/hER9tZcxgctr5teDMmh9Q31uJflrbDEK1G6MQ2B5rLSFTYg7IOuN5Jq5ND7+MzHdmcoou+tl/fU4rfEZh7Tf5laW/Atz1sGYlNnGguM33Yh+iAUON+A53AcOj3+FRHW/MwhFxTf0orA8q3avIvS3sLhaMdrdQWhCMGmfPfYNx+9dx0MEtF5hswVy69w+c6Sl12gg8O84BmX5N/WZpWUx90CR3EOhl8Yx+PlAGgka5HvNx9HzJuOYP3SXj0fB0pcA+Ark+WADt1Pf5FaTbfd4/P/B78lnejNkSpgbeHMmVtZ27M5Uu+AQwRHPoaOkrfO6FeCOvBVLsef4y0r0GSvF+YHk55r5NAfVSDnoc2aThqYgrfbKnHHyPtv8PMVBOX/kgIZmZyZWlfRE+o8I6gPxI9KzO5trSv4sFcG1z64zCfOrNzpX0VLbPC49IfBdQfvf6otK/j6L+X/Aba7Gn/b0u7BOLG/rn0tbGcntev5K4h7TIIjw/Ppq+L8dTZnCvtQuBurNfZmMWpuVVC2j9vCe9moRimkgAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-5px;" /> (where <img width="23" alt="W&39;" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC4AAAAaCAQAAABiW6YHAAABZUlEQVR4nLWVa7nDIAyGsYCFWsACFrBQC7WAhVqohVqYhVqYBU5oKSW3PbRn+/izBXhpQhKM+b68GX5A3TWaZN7YNJgVjVhnZjKj7bF1fTIbhltwJoA5jwi/XZ3xMF67fYEVHu0JYMvfGRv7GyyT5NIKE6tgjztciqQHmEP/82FWWLnDXyrcizOBrYzCugJPgn1S4BbCgrUpHprjMjjc7lsk+IxCkq84seOqogiPxU6vyQEca1KC18Dx12TXfckirIVd3EKTkMPx2TM4awV4YIflVaMO9wzuCoLDeco6WpkSvE2u0/VEKoCmYIdobEN1c0NwnoIdGgj8QuDapSnYqRbedoy1aUdOq8Ee+PG9FmVxW7s8BW/AD/dnVMZXeQW55/XojC11/YJLXfMGPPdF6vpZu1Ev7z54alLw1FjsD1Lw0txcaaujArb/Pb3asyC3rgdwyfUPz9c9uOy68uzekVMRD67yD8wvuOEiRwklAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-1px;" /> is the weight matrix of the second layer). This is shown below, where we compose three such parametric maps (and use <img width="18" alt="P_1" height="14" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAcCAQAAACjINXTAAAA70lEQVR4nN2V3RGEIAyE04It2IIt0IIt0AIt0AIt2AIt2IIt2IIu5P7mhBDGuZfbvDiZ+EEWokRXLRSL4cnRWKivyiA8HYglP3PMyO3I+R5UgiWQ+coOtCIbekC8o6tscQFBEeWxkOeddrRXK2eQurmyQ0ncstOCag5RNvvQX4IfO9R9+GWHDDC73p0k/wA9wyGTIKFvQNihDS+/w9LUh2DpDB0xiaLYoVmomNBsyAMsih0ahAqLZcZsgKhUsMolrzpR2ivXALUdUoLaDilBWocaIG7sFog/oZ/RmqrmqWn1x6D1PshjN/zt3vKvPOsETT6PLW9RXjsAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-3px;" />, <img width="18" alt="P_2" height="14" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAAcCAQAAABM4r7tAAABIElEQVR4nLVVYRmFIAxcBStYgQpWsAIVrEAFK1CBClZ4FazgO5w+URk4v+/d/ugcx3ZuQHSHp5A1RwO1mfgCOpijBebXZ7Yevhk+pyOLdJGqu3gbmuAddVSc1R02u0URAQtCxs/ZqoqUFjCVosS8UhFc+PCcSlKKVtkXTUv8XakXrZBXqgPRrFEpwm1Uuw3wRJpROzas1AfLD7NktCSMurQdMgyYUAv9imGRqi9ExDxb2tUr5MtKybv1ScYtImc5Nio1FXLy+GqStwVlCqgpFU7fnRxdV8qgOY6SRpmqptQVszyRNaXuGwv9z+U9pTL5ieSDNrXatBkUJ/47DRqMluqEl4nCr7eM/jpLiXzSpLbYOhWiCXbc2cvbU4NH5Wwbvu9GnkCdzHw/AAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-3px;" />, and <img width="18" alt="P_3" height="14" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAAdCAQAAACHvm1IAAABM0lEQVR4nLVVURXDMAjEQi3EwizUwizUQizEQizUQi3UwizEQgdhbZY10ND3dvzsMbjAFRKAM2ZYmhbAg2vEKxjRAmxoc/7N9kRfQl+wkREdUY0/3gFW9EYbFVd1xtQ8QsWCCUvDz9WampQSmMrQYlspAjfu+6kkpSDLvllG4u9K3RiFtlIjEiWLSoTwodrNo4doonVtWKkXpheb4GElYVxLSzsacUM96qeGEdVTiYgHRcS2lSVipeTTHpi+1zxgZJKpSKlVqWnMSu6QRvn4U1dqOr6kq2gbZ+pKfSNosVdKFTgUP2GFIq6UKkfS2C7y9+P2eqgYU0tXvmi/rW/bVoOyJ7hqiUJ+l24iVXUE4Wbrpipiz3dexlJHSXVInLpGRySL2CK91zQO5jusxpCffV/fYm8g5J2FD8eWQgAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-3px;" /> to denote weight spaces).</p>
<center>
<img src="../images/architecture_agnostic.png" alt="Composition of three feedforward layers" width="600" />
</center>
<p>But a GCNN layer is different! It is defined as a function <img width="109" alt="X \mapsto \sigma(AXW)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANoAAAAiCAQAAACT1Q8kAAAEtElEQVR4nOVa25XjIAx1C7TgFtwCLfhvv92CW6AFznbgFtyCW3ALbiHDy8NLgORsZuLs9ZmPcQiIKyFdIF336Rg6+dsmXAbrFvX332HsNuS0p0682JYrGHD2992aPB48ev+Ok4wxdLuaDwZ9d0QzTT+lcZK290zJYj/xd7yjZuW2JpgyY1Rfe5hHqP/Czybz9lDvR0XJe4Mpl03ItouaVZkcKie2vW/tmeLq2cz7RbWI+xmNFUfSf4dfHr3pGJrG4zZ5VlTWTgzuCK6Dyol2MmSBMP1AGYArl+VLQY/LgdYAFtN12ljepqwTpqocsSCcRuVkLbhYgL3YT0awJ4lJkRo2+mJzxG1cpie6I1sK8+jZthI+jZO1EAhzwWlaK8IYVHtkot+TZTzdyGVMWT6jWvYmiktEpqBwIkGnMddHPpasBM2GTfWTK6Rt854BOoZImAtVI8di6LNrCE5OISicCNBp56pOQ6q+nxTo+aiyqNWMNQ+ZVclYXxIMKzI5cpeSeOSMGvCcQCmXmSCBxqrLO47OHG7YychUmmLE7+FmRQFdjQ5GPMdP2MujWB9ibC5+GdppeE4gwSHVeNBYY3Ns7IycyN2jzR4GnKDcmNmZUCBcVUgfH4kc7QDfKpcYMPCc8Mxpgxsvd1q7Ym1oYeWKaTvXx6A4TVN3YPO1ImozQloY1WY3qVb9zdEpAmb8PkpvR2FXlQPLSV4nzxT4SMYqSf0QK2JL4jChIzAEzWk6inDp17rMC5epsn1tj79EbUpb4RxYTtLaNX5bvkdjlaV+CNycOktSKnIxoDqNGbe1x1gzrbmDti2oPVdMVWlXlQLPSZ84zY8XB0hN6nsgncZUx0MicnGgOs1ScTT00QQU4xUcCZNK0iCBBXpuJ4WTsFV4oriaqmgxILlFOc2aZ4XCA18EDf6AQgHz7JWo24H1c9Vpwpzqhw9m9VA5eXyHGYvSabiqsSe5IssygHnbd5NT5OKhV9pfQJTXnsWkjHJBHjvoJO+45DQtQdLxZab1UtA58YJDRuHgV/WI3X2Z7zTkyhIY1GdqpwV6epRda/MogISkLTuAttpptfhdAOtygZ5/i8rJWbvSFOidhme1mR5lEkP2dBt/f0Z1mgCvJGKsQKTNBRVXn2AqQc639Tp1hZPznD9NgedZSXpzVkPDaTIzHTrdroHmNI5KvlD1grVja4IlnVpz2jVObO0as9lN7j3yjMNgqVVc+HbIlmnsuQjNaevFfcpYJHmsJFtRdEzZCVc5Obf/KbgTXZSNVKVOy0KWpQl/Rjj44k3N5tuFEVv7VUdftHWoSJTSmch1TkrXnfjj6dA6wIpeGbEXzGZuePxxEx4lSqCW/txE/0pJVgJjB3pl5nC6VIe4k/Fhn89yIsB1Zp1GOyYHwrBPdktxd3PyKUVHttHaUocQRnSP5la6Ln9lNos1mEFK5JLMUFv0Lzgp34KhL1ocJpIQfDk4KeKY2VO11zsHtObPYyi6hiJBbHva8cZNsZOJeV8wcgW8KSaC2n13TJcuim+J7WOic/+YmTQxvETt/jxQPwz/HOB/Y/y+GKo3Hx+J/PjpXtA3C7+vgn8c6UHvvbDc2voncN/kwuCa/AUJre+cN/VtGQAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-5px;" />, where in addition to the weight matrix <img width="20" alt="W" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAYCAQAAAAijXdLAAABN0lEQVR4nJWVbRmFIAyFV4EKVqACFahgBStQgQpWsIIVrHAreCcflw0Gcg9/eIZ7ZXKYAFELHGw4yPLVShHPUcCkwICFOwyHc/1bMTjOEN/xCcNyLMZu+IQMUQcuH0LcBeAirBjEaSFOgGcXKO3C4S4HOkJiq60DVFjyUF4EKrg6QD8q95ETgS7FtyquEfiimMjf+pRl0ulz7bVVekBemsfTVQLQNi8QZBqgTmktULJXB0iNkMu6K4e+2IUDy04srGl2MeCrXbKWCljS+B16tUsRBdIb+gCvNNczx0GBcV+KuYzeoQm7cGAszbNmUCxvG4MPlb9VXVYBTtmFA59+U5eV71C3842AN7FL1prik3Yp8uRYqKJDL7HJDtVrpXJ7mARKZZnw7/jDLhQol9X2wynpbtrkcXwBuGOjXTaKpuwAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> we also have the adjacency matrix <img width="14" alt="A" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB0AAAAYCAQAAACsVz43AAAAyElEQVR4nJ2UYRWDMAyEYwELtYCFWJiFWcBCLGChFmqhFrBQC1sSKK90g6a9/CP5HvR6AcCmN5BxspKDBGEM9fCBOAIig1IDivrWAZS0BJ37QKdnXBTFPtQrsJ/21QMio3CiXTcb+YNFUy9KxbCgqxV0lxAkRs158hdHgx3NBpWoMRTZoCyyoqSbUtamqGuBYhBWtdry5P+MoAWtDSrRRihqg7KaKN22G3maHy7gIU8Tb2W6XWjU3sZTPwrH/0eqtskXPallf/wFfgxlrCF298UAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-0px;" /> of our underlying graph.</p>
<center>
<img src="../images/gcnn_formula.png" alt="A GCNN layer" width="600" />
</center>
<p>Crucially (and unlike with standard feedforward layers) here each layer <em>shares</em> the adjacency matrix. This means that when we compose two layers we obtain a composite morphism <img width="165" alt="X \mapsto \sigma(A\sigma(AXW)W&39;)" height="17" src="data:image/png;base64,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" class="inline-math" style="margin:0; vertical-align:-5px;" />. Graphically, this is shown below, where we see a composition of three GCNN layers.</p>
<center>
<img src="../images/graph_convolutional_blog_post.png" alt="Composition of three GCNN layers" width="600" />
</center>
<p>Note that here <img width="14" alt="A" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB0AAAAYCAQAAACsVz43AAAAyElEQVR4nJ2UYRWDMAyEYwELtYCFWJiFWcBCLGChFmqhFrBQC1sSKK90g6a9/CP5HvR6AcCmN5BxspKDBGEM9fCBOAIig1IDivrWAZS0BJ37QKdnXBTFPtQrsJ/21QMio3CiXTcb+YNFUy9KxbCgqxV0lxAkRs158hdHgx3NBpWoMRTZoCyyoqSbUtamqGuBYhBWtdry5P+MoAWtDSrRRihqg7KaKN22G3maHy7gIU8Tb2W6XWjU3sZTPwrH/0eqtskXPallf/wFfgxlrCF298UAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-0px;" /> is globally accessible information for each layer, while <img width="20" alt="W" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAYCAQAAAAijXdLAAABN0lEQVR4nJWVbRmFIAyFV4EKVqACFahgBStQgQpWsIIVrHAreCcflw0Gcg9/eIZ7ZXKYAFELHGw4yPLVShHPUcCkwICFOwyHc/1bMTjOEN/xCcNyLMZu+IQMUQcuH0LcBeAirBjEaSFOgGcXKO3C4S4HOkJiq60DVFjyUF4EKrg6QD8q95ETgS7FtyquEfiimMjf+pRl0ulz7bVVekBemsfTVQLQNi8QZBqgTmktULJXB0iNkMu6K4e+2IUDy04srGl2MeCrXbKWCljS+B16tUsRBdIb+gCvNNczx0GBcV+KuYzeoQm7cGAszbNmUCxvG4MPlb9VXVYBTtmFA59+U5eV71C3842AN7FL1prik3Yp8uRYqKJDL7HJDtVrpXJ7mARKZZnw7/jDLhQol9X2wynpbtrkcXwBuGOjXTaKpuwAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> and <img width="23" alt="W&39;" height="13" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC4AAAAaCAQAAABiW6YHAAABZUlEQVR4nLWVa7nDIAyGsYCFWsACFrBQC7WAhVqohVqYhVqYBU5oKSW3PbRn+/izBXhpQhKM+b68GX5A3TWaZN7YNJgVjVhnZjKj7bF1fTIbhltwJoA5jwi/XZ3xMF67fYEVHu0JYMvfGRv7GyyT5NIKE6tgjztciqQHmEP/82FWWLnDXyrcizOBrYzCugJPgn1S4BbCgrUpHprjMjjc7lsk+IxCkq84seOqogiPxU6vyQEca1KC18Dx12TXfckirIVd3EKTkMPx2TM4awV4YIflVaMO9wzuCoLDeco6WpkSvE2u0/VEKoCmYIdobEN1c0NwnoIdGgj8QuDapSnYqRbedoy1aUdOq8Ee+PG9FmVxW7s8BW/AD/dnVMZXeQW55/XojC11/YJLXfMGPPdF6vpZu1Ev7z54alLw1FjsD1Lw0txcaaujArb/Pb3asyC3rgdwyfUPz9c9uOy68uzekVMRD67yD8wvuOEiRwklAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-1px;" /> are layer specific. This means that we need a way to account for the global information, in addition to having <strong>Para</strong> account for the local one. Fortunately, there is a neat categorical construction that models just that – the already mentioned CoKleisli category of the product comonad on some base category <img width="11" alt="\mathcal{C}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABcAAAAZCAQAAABwKX1bAAAAzklEQVR4nL2TWxXDIAyGYwELWMACFrCABSzEwizMAhZmAQtYYClj7RYSur7szzk8NB+5NAFgJQseEBK4JUUyhBVou+EKRqjwoBP3S1lDHYGZyjgK2vCbBlfmeuFRg3naSHCRYEOfK53fylqjKDi8FntzzLHvZFbC1aSSXO//dHpvoVajrKKPYpY934tPhY77X3H8Ax6u4VqrgeeNHZf+u+nTZuOz44klMa+w67njlcWJq4fRxjPzo+JM5albdFxoIxNO68zaSrTdmQzlEgCe31FnmZGs1A4AAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" />. It’s defined for a particular object in our category (the space <img width="37" alt="\mathbb{R}^{n \times n}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEsAAAAYCAQAAAAYXcg4AAABwUlEQVR4nM1Wcd2EIAylAhWscBWoYAUrWIEKVrCCFahwFazgveHgA4Tp/XH6jd95CLztwcamUr8RjXaU7jslvVqazaqhakKWUbkDalLzd0o6GLdq9SSobbG3ok+/qdip9kZ63xvRX/AsSeTE8vdzfBSLaRIDGjrCHcZfULHimRp5QemGk5z9uEZfInYkeY6PtJZI608MCJEM+A8npr07SO07GnOMrhErSV3FC7QU3oz/J8cGqoNfH2b2VVNFJxEqSX2DF2gtDO6hPhWHvQbZHVKTN2bqN/AaXqA1VMZ1tr+pYbx+WtfxIq3aaeX7W/nymxTYjK2reJFWiIA5875N9jewiSFzhHQTr+BFWiFdpDeRJI0My2vmxLSct87xIq0O8BmUFpDqs9VpZHSYtZyBdjnP8jK+Qcv6zOvQ23x2sQcjuQpdREW9JqZjMr5J6+3zO5WfxsW9S3InGo6lMcnE/4AWBeD+7mJ2f0RKWpod+MqKxOO0KOWt3oH2SUfWEsTMl9q1CunvpZ639nzVPXcj61l+ZEf2xYfgjbRCuSxLteX524kZNIof4z/RqHCaWAE7fqcVZQn6qYxcZPLmGvM3EPsAWxUMrS8VZRYAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-0px;" /> of adjacency matrices of a specific size) and its looks at coeffectful morphisms that have access to this information. It might sound complicated, but it has a simple graphical representation. Below we see a composition of three morphisms in this category.</p>
<center>
<img src="../images/cokl_composition.png" alt="Composition of morphisms in the CoKleisli category of the product comonad." width="600" />
</center>
<p>We’ll denote this category by <img width="116" alt="\mathsf{CoKl}(\mathbb{R}^{n \times n} \times -)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOkAAAAiCAQAAAAQETYfAAAE+ElEQVR4nO2bC3WsMBCGsRALWMBCLGABC1iIBSxgYS1gAQtY2OYJefwDoV0ge05/zr0tC2TDfMlkZqBVVbKaani6CwWLVaP8d0J11VZCblxuz6itpnNdPiEGW64v+rar1ORbqK/m6h1sL2ngfJkrcCcWeWyRP42EPhefOVsTt7ItahNV9yvsPTDGIEf9t0ndx6GaFaczm9k7c7sU0joCSiNlshfdepXyFosGKPQV7jfT2lvCqKOrFZ5W/9bL31/y/1hDBDXcz2mhDClb7MrMorc8zb+9Thr44MJAGCmTRguB0khFcr2w6yqX57u+qRaFbG+MWh3kXivP6+SRRp/3PoCaAs5poQTVsmc7S2PsFjfhtYcSQoqAUkhRNx1kHpzPZYtKnfzpZirTHkUBmddeT9BrOJAx0PwWStCw53wnbeC/h0MpUmbbjldkjHSQpoyFkVZrfzcHxLXLFsGdvInYWZkjBnquhefVaG8C1WnzfqLjMVIHNP1ihBQ7OQrpy5q9jcbq5A2Llr5pHTngSDe3hedFehATFn0ikI+RUkAx0h72gkbagc9ZMDgH8r7wLD3Tgq89/8bAcvYpCdy75nRcqzJWfJsh0oEEipG+gNs9O0vDWbXY+4pNTq2l+S2EWkgfp/zUsnvtX8Rx8CZ2DB+rljfoMtYJZKw+0j2gGCkeWEdr6RiY0x+3nf3+LurFXsSb10KshoBqgF43SwmbjdmhUWcTHZWzLnD93ZAOFjx1OylSrlModCaOeE1K40e8Sv46KOw5Y0Xnoen+cQtICOr1QMPernoRGWIsbh20u7ne5rG+HFIH9E2G2SnSnhhYCGkt2x0lTjW0Ql/hDzNV4BA2v9y+5ah6dNQCpRjqHUANPfjhMVKmg6hwknfJPDRIhZ3BAkB3SpGKDKRC13MmO1zmoDBiFJqQJS3iPNv/7KgFWj7Ue4ASVstD2sGoeIqcrzpnc8nMxtLoxlKkI3mmj3TWdSNVEiwxtXBQ7wJKIB2y1tKxQqW+GIxztw4zJ51vihS6kCp2vNyunb1X3ylJBupdQAmkZk08qmMu0IXyaDik6yflfH+LVA0usz+dqj7fJ74bFqZq1omwv+FUSFQgq8jLSzEYhDScPcb5LonL/j1SZp1uk+Fb7pdxuXSeisQzNyRlR/D4M6d6lI80ds8cfoqRIleaRrytNBnTR0pzvm4NpfLUz4sIKruMefqCa6IBs5kVP1wbgGv/XcTrNNreTkWV1P2g6C6olNVs1LsXQwo4k+cIIfW8dEmc79+Q1jYfrQuKfOMo9x6oI+Vfa5t64AK7ksES4jKBle/Jqbca2uRIirQlgjRcPeqt821viy33hdKWO6BSEUi1vdXgv/ph3mpwlzj37I67goIv+t2jMXK+KdIartaVrt4opWV7YY8/D5XKQ6+Huuw9oK/Xt41mXcWd1z0HsbefbDXeVxSe0Ehj54vK9jMIrbheL7l+OK28yPYMqLb7XJvzzEtvnxedh14LlZoGnroVq8MZumLuHY+PKdFInfN1QRhCOkQxbx/0Ja0ah8efhDrs+Ikr30lWwzzDQ7E1D8JpTa2PXeHq+MNgvk8jfMJclOYvfKP2ObFjt/u8OqLc8C+kzkb9havUym2JOveO9WNqQD34X0hZf0JRhtL37f+VSv25y9PZ+AkN3+FQHhSDL/MVraGYym2ZGr/RPl/kVG4Xw9HGD+YOULg5ycNPAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-5px;" />. (Un)surprisingly, it turns out that when the category <img width="11" alt="\mathcal{C}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABcAAAAZCAQAAABwKX1bAAAAzklEQVR4nL2TWxXDIAyGYwELWMACFrCABSzEwizMAhZmAQtYYClj7RYSur7szzk8NB+5NAFgJQseEBK4JUUyhBVou+EKRqjwoBP3S1lDHYGZyjgK2vCbBlfmeuFRg3naSHCRYEOfK53fylqjKDi8FntzzLHvZFbC1aSSXO//dHpvoVajrKKPYpY934tPhY77X3H8Ax6u4VqrgeeNHZf+u+nTZuOz44klMa+w67njlcWJq4fRxjPzo+JM5albdFxoIxNO68zaSrTdmQzlEgCe31FnmZGs1A4AAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> has enough structure to do learning on it, so does <img width="116" alt="\mathsf{CoKl}(\mathbb{R}^{n \times n} \times -)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOkAAAAiCAQAAAAQETYfAAAE+ElEQVR4nO2bC3WsMBCGsRALWMBCLGABC1iIBSxgYS1gAQtY2OYJefwDoV0ge05/zr0tC2TDfMlkZqBVVbKaani6CwWLVaP8d0J11VZCblxuz6itpnNdPiEGW64v+rar1ORbqK/m6h1sL2ngfJkrcCcWeWyRP42EPhefOVsTt7ItahNV9yvsPTDGIEf9t0ndx6GaFaczm9k7c7sU0joCSiNlshfdepXyFosGKPQV7jfT2lvCqKOrFZ5W/9bL31/y/1hDBDXcz2mhDClb7MrMorc8zb+9Thr44MJAGCmTRguB0khFcr2w6yqX57u+qRaFbG+MWh3kXivP6+SRRp/3PoCaAs5poQTVsmc7S2PsFjfhtYcSQoqAUkhRNx1kHpzPZYtKnfzpZirTHkUBmddeT9BrOJAx0PwWStCw53wnbeC/h0MpUmbbjldkjHSQpoyFkVZrfzcHxLXLFsGdvInYWZkjBnquhefVaG8C1WnzfqLjMVIHNP1ihBQ7OQrpy5q9jcbq5A2Llr5pHTngSDe3hedFehATFn0ikI+RUkAx0h72gkbagc9ZMDgH8r7wLD3Tgq89/8bAcvYpCdy75nRcqzJWfJsh0oEEipG+gNs9O0vDWbXY+4pNTq2l+S2EWkgfp/zUsnvtX8Rx8CZ2DB+rljfoMtYJZKw+0j2gGCkeWEdr6RiY0x+3nf3+LurFXsSb10KshoBqgF43SwmbjdmhUWcTHZWzLnD93ZAOFjx1OylSrlModCaOeE1K40e8Sv46KOw5Y0Xnoen+cQtICOr1QMPernoRGWIsbh20u7ne5rG+HFIH9E2G2SnSnhhYCGkt2x0lTjW0Ql/hDzNV4BA2v9y+5ah6dNQCpRjqHUANPfjhMVKmg6hwknfJPDRIhZ3BAkB3SpGKDKRC13MmO1zmoDBiFJqQJS3iPNv/7KgFWj7Ue4ASVstD2sGoeIqcrzpnc8nMxtLoxlKkI3mmj3TWdSNVEiwxtXBQ7wJKIB2y1tKxQqW+GIxztw4zJ51vihS6kCp2vNyunb1X3ylJBupdQAmkZk08qmMu0IXyaDik6yflfH+LVA0usz+dqj7fJ74bFqZq1omwv+FUSFQgq8jLSzEYhDScPcb5LonL/j1SZp1uk+Fb7pdxuXSeisQzNyRlR/D4M6d6lI80ds8cfoqRIleaRrytNBnTR0pzvm4NpfLUz4sIKruMefqCa6IBs5kVP1wbgGv/XcTrNNreTkWV1P2g6C6olNVs1LsXQwo4k+cIIfW8dEmc79+Q1jYfrQuKfOMo9x6oI+Vfa5t64AK7ksES4jKBle/Jqbca2uRIirQlgjRcPeqt821viy33hdKWO6BSEUi1vdXgv/ph3mpwlzj37I67goIv+t2jMXK+KdIartaVrt4opWV7YY8/D5XKQ6+Huuw9oK/Xt41mXcWd1z0HsbefbDXeVxSe0Ehj54vK9jMIrbheL7l+OK28yPYMqLb7XJvzzEtvnxedh14LlZoGnroVq8MZumLuHY+PKdFInfN1QRhCOkQxbx/0Ja0ah8efhDrs+Ikr30lWwzzDQ7E1D8JpTa2PXeHq+MNgvk8jfMJclOYvfKP2ObFjt/u8OqLc8C+kzkb9havUym2JOveO9WNqQD34X0hZf0JRhtL37f+VSv25y9PZ+AkN3+FQHhSDL/MVraGYym2ZGr/RPl/kVG4Xw9HGD+YOULg5ycNPAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-5px;" />. Which in turn means that all the constructions for parametric bidirectional morphisms from our original paper apply here as well! For instance, we can use the same kind of diagrams (drawn below) depicting a closed parametric lens whose parameters are being learned (from our <a href="https://arxiv.org/abs/2103.01931">original paper</a>) for the base category <img width="116" alt="\mathsf{CoKl}(\mathbb{R}^{n \times n} \times -)" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOkAAAAiCAQAAAAQETYfAAAE+ElEQVR4nO2bC3WsMBCGsRALWMBCLGABC1iIBSxgYS1gAQtY2OYJefwDoV0ge05/zr0tC2TDfMlkZqBVVbKaani6CwWLVaP8d0J11VZCblxuz6itpnNdPiEGW64v+rar1ORbqK/m6h1sL2ngfJkrcCcWeWyRP42EPhefOVsTt7ItahNV9yvsPTDGIEf9t0ndx6GaFaczm9k7c7sU0joCSiNlshfdepXyFosGKPQV7jfT2lvCqKOrFZ5W/9bL31/y/1hDBDXcz2mhDClb7MrMorc8zb+9Thr44MJAGCmTRguB0khFcr2w6yqX57u+qRaFbG+MWh3kXivP6+SRRp/3PoCaAs5poQTVsmc7S2PsFjfhtYcSQoqAUkhRNx1kHpzPZYtKnfzpZirTHkUBmddeT9BrOJAx0PwWStCw53wnbeC/h0MpUmbbjldkjHSQpoyFkVZrfzcHxLXLFsGdvInYWZkjBnquhefVaG8C1WnzfqLjMVIHNP1ihBQ7OQrpy5q9jcbq5A2Llr5pHTngSDe3hedFehATFn0ikI+RUkAx0h72gkbagc9ZMDgH8r7wLD3Tgq89/8bAcvYpCdy75nRcqzJWfJsh0oEEipG+gNs9O0vDWbXY+4pNTq2l+S2EWkgfp/zUsnvtX8Rx8CZ2DB+rljfoMtYJZKw+0j2gGCkeWEdr6RiY0x+3nf3+LurFXsSb10KshoBqgF43SwmbjdmhUWcTHZWzLnD93ZAOFjx1OylSrlModCaOeE1K40e8Sv46KOw5Y0Xnoen+cQtICOr1QMPernoRGWIsbh20u7ne5rG+HFIH9E2G2SnSnhhYCGkt2x0lTjW0Ql/hDzNV4BA2v9y+5ah6dNQCpRjqHUANPfjhMVKmg6hwknfJPDRIhZ3BAkB3SpGKDKRC13MmO1zmoDBiFJqQJS3iPNv/7KgFWj7Ue4ASVstD2sGoeIqcrzpnc8nMxtLoxlKkI3mmj3TWdSNVEiwxtXBQ7wJKIB2y1tKxQqW+GIxztw4zJ51vihS6kCp2vNyunb1X3ylJBupdQAmkZk08qmMu0IXyaDik6yflfH+LVA0usz+dqj7fJ74bFqZq1omwv+FUSFQgq8jLSzEYhDScPcb5LonL/j1SZp1uk+Fb7pdxuXSeisQzNyRlR/D4M6d6lI80ds8cfoqRIleaRrytNBnTR0pzvm4NpfLUz4sIKruMefqCa6IBs5kVP1wbgGv/XcTrNNreTkWV1P2g6C6olNVs1LsXQwo4k+cIIfW8dEmc79+Q1jYfrQuKfOMo9x6oI+Vfa5t64AK7ksES4jKBle/Jqbca2uRIirQlgjRcPeqt821viy33hdKWO6BSEUi1vdXgv/ph3mpwlzj37I67goIv+t2jMXK+KdIartaVrt4opWV7YY8/D5XKQ6+Huuw9oK/Xt41mXcWd1z0HsbefbDXeVxSe0Ehj54vK9jMIrbheL7l+OK28yPYMqLb7XJvzzEtvnxedh14LlZoGnroVq8MZumLuHY+PKdFInfN1QRhCOkQxbx/0Ja0ah8efhDrs+Ikr30lWwzzDQ7E1D8JpTa2PXeHq+MNgvk8jfMJclOYvfKP2ObFjt/u8OqLc8C+kzkb9havUym2JOveO9WNqQD34X0hZf0JRhtL37f+VSv25y9PZ+AkN3+FQHhSDL/MVraGYym2ZGr/RPl/kVG4Xw9HGD+YOULg5ycNPAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-5px;" />, and draw string diagrams of GCNN learning!</p>
<center>
<img src="../images/closed_supervised.png" alt="String diagram of a closed supervised learner." width="800" />
</center>
<p>I find this pretty exciting.</p>
<p>On the other hand, I see this paper merely as a stepping stone. It’s a neat idea - but there’s much more work to do. What’s making me deeply excited is where it could lead us further – to categorical models of full message passing Graph Neural Networks – and beyond.</p>
<p><br> <br></p>
<p>Thanks to <a href="https://ievacepaite.com/">Ieva Čepaitė</a> for a read-through of this post.</p>


<form style="border:1px solid #ccc;padding:3px;text-align:center;" action="https://tinyletter.com/bgavran" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/bgavran', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
    <p><label for="tlemail">Want to subscribe to my newsletter? Sign up with your email below.</label> Or subscribe via <a href="../rss.xml">RSS</a>. </p>
    <p><input type="text" style="width:140px" name="email" id="tlemail" /> <input type="hidden" value="1" name="embed" /><input type="submit" value="Sign up" /></p>

    <small> <a href="https://tinyletter.com" target="_blank">powered by TinyLetter</a> </small>
</form>

<div class="commentbox"></div>


<script>
commentBox('5769281040023552-proj');
</script>

        <div id="footer">
          <div class="inside">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
            This theme was designed by <a href="http://twitter.com/katychuang">Dr. Kat</a> and showcased in the <a href="http://katychuang.com/hakyll-cssgarden/gallery/">Hakyll-CSSGarden</a>


          </div>
        </div>

          </div>
        </div>
    </body>
</html>
