<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PWZE1HYS87"></script>
        <script>
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
         gtag('js', new Date());

         gtag('config', 'G-PWZE1HYS87');
        </script>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="author" content="Bruno Gavranovic" />
        <meta name="viewport" content="width=device-width" />
        <meta http-equiv="Cache-Control" content="max-age=86400, must-revalidate" />
        <title>Graph Convolutional Neural Networks as Parametric CoKleisli morphisms</title>
        <script src="../css/jquery.js"></script>
        <script src="../css/selectfile.js"></script>
        <link rel="stylesheet" type="text/css" title="hakyll_theme" href="../css/theprofessional.css" />
        <link href="https://fonts.googleapis.com/css?family=Titillium+Web" rel="stylesheet">

        <!-- MathJax font size adjustment -->
        <style>
            mjx-container {
                font-size: 80% !important;
            }

            /* Scale TikZ diagrams uniformly */
            .page svg {
                transform: scale(1.2) !important;
                transform-origin: center !important;
            }

            /* Improve code blocks */
            pre, code {
                background-color: #f8f9fa;
                border: 1px solid #e1e4e8;
                border-radius: 3px;
                font-family: 'Monaco', 'Menlo', 'Courier New', monospace;
                font-size: 0.9em;
            }

            pre {
                padding: 1em;
                overflow-x: auto;
            }

            code {
                padding: 0.2em 0.4em;
            }

            pre code {
                background: none;
                border: none;
                padding: 0;
            }
        </style>

        <!-- MathJax for regular math -->
        <script>
            MathJax = {
                tex: {
                    macros: {
                        coloneqq: '\\mathrel{\\vcenter{:}}=',
                        enskip: '\\hspace{0.5em}'
                    }
                }
            };
        </script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

        <!-- TikZJax for TikZ diagrams -->
        <link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
        <script src="https://tikzjax.com/v1/tikzjax.js"></script>
    </head>
    <body>
        <div class="highbar">&nbsp;</div>
        <div class="container-gallery">
        <div id="content" class="inside">

        <div id="header">
          <div class="box">
            <div id="logo" class="name">
                <h2><pageTitle><a href="../">Bruno Gavranović</a></pageTitle></h2>
            </div>
            <div id="navigation" class="pageslinks">
              <nav class="menuNav">
                <div class="menuItems">
                <a href="../" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Home</a>
                <a href="../archive.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Posts</a>
                <a href="../papers.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Papers</a>
                <a href="../research_programme.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Research Programme</a>
                <a href="../about.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">About</a>
		<!-- <a href="/contact.html" class="posts/2022-12-05-graph_neural_networks_as_parametric_cokleisli_morphisms.md">Contact</a> -->
                </div>
              </nav>
            </div>
        </div>
        </div>
            <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>

<div class="info">
    Posted on December  5, 2022
    
</div>

<h1 id="graph-convolutional-neural-networks-as-parametric-cokleisli-morphisms">Graph Convolutional Neural Networks as Parametric CoKleisli morphisms</h1>
<p>This is a short blog post accompanying the latest preprint of Mattia Villani and myself which you can now find <a href="https://arxiv.org/abs/2212.00542">on the ArXiv</a>.</p>
<center>
<img src="../images/abstract_transparent.png" alt="Abstract of the paper" width="600" />
</center>
<p>This paper makes a step forward in substantiating our existing framework described in <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a>. If you’re not familiar with this existing work - it’s a general framework for modeling neural networks in the language of category theory. Given some base category with enough structure, it describes how to construct another category where morphisms are <em>parametric</em>, and <em>bidirectional</em>. Even more specifically - it allows us to describe the setting where the information being sent backwards is the derivative of some chosen loss function.</p>
<p>This is powerful enough to encompass a variety of neural network architectures - recurrent, convolutional, autoregressive, and so on. What the framework doesn’t do is describe the structural essence of all these architectures at the level of category theory.</p>
<p>Our new paper does that, for one specific architecture: Graph Convolutional Neural Networks (GCNNs). We show that they arise as a morphisms for a particular choice of the base category - the CoKleisli category of the product comonad.</p>
<p>It’s a pretty straightforward idea. It’s based on the central observation that a GCNN layer has a different composition rule than just a regular feedforward layer. A simple feedforward layer is often thought of as a map <span class="math inline">\(X \mapsto \sigma(XW)\)</span>, where <span class="math inline">\(W\)</span> is the weight matrix of that layer, and <span class="math inline">\(\sigma\)</span> is some activation function. Then two layers are composed using the <a href="https://ncatlab.org/nlab/show/para+construction"><strong>Para</strong></a> construction which defines a map <span class="math inline">\(X \mapsto \sigma(\sigma(XW)W')\)</span> (where <span class="math inline">\(W'\)</span> is the weight matrix of the second layer). This is shown below, where we compose three such parametric maps (and use <span class="math inline">\(P_1\)</span>, <span class="math inline">\(P_2\)</span>, and <span class="math inline">\(P_3\)</span> to denote weight spaces).</p>
<center>
<img src="../images/architecture_agnostic.png" alt="Composition of three feedforward layers" width="600" />
</center>
<p>But a GCNN layer is different! It is defined as a function <span class="math inline">\(X \mapsto \sigma(AXW)\)</span>, where in addition to the weight matrix <span class="math inline">\(W\)</span> we also have the adjacency matrix <span class="math inline">\(A\)</span> of our underlying graph.</p>
<center>
<img src="../images/gcnn_formula.png" alt="A GCNN layer" width="600" />
</center>
<p>Crucially (and unlike with standard feedforward layers) here each layer <em>shares</em> the adjacency matrix. This means that when we compose two layers we obtain a composite morphism <span class="math inline">\(X \mapsto \sigma(A\sigma(AXW)W')\)</span>. Graphically, this is shown below, where we see a composition of three GCNN layers.</p>
<center>
<img src="../images/graph_convolutional_transparent.png" alt="Composition of three GCNN layers" width="600" />
</center>
<p>Note that here <span class="math inline">\(A\)</span> is globally accessible information for each layer, while <span class="math inline">\(W\)</span> and <span class="math inline">\(W'\)</span> are layer specific. This means that we need a way to account for the global information, in addition to having <strong>Para</strong> account for the local one.
Fortunately, there is a neat categorical construction that models just that – the already mentioned CoKleisli category of the product comonad on some base category <span class="math inline">\(\mathcal{C}\)</span>. It’s defined for a particular object in our category (the space <span class="math inline">\(\mathbb{R}^{n \times n}\)</span> of adjacency matrices of a specific size) and its looks at coeffectful morphisms that have access to this information.
It might sound complicated, but it has a simple graphical representation. Below we see a composition of three morphisms in this category.</p>
<center>
<img src="../images/cokl_composition.png" alt="Composition of morphisms in the CoKleisli category of the product comonad." width="600" />
</center>
<p>We’ll denote this category by <span class="math inline">\(\mathsf{CoKl}(\mathbb{R}^{n \times n} \times -)\)</span>.
(Un)surprisingly, it turns out that when the category <span class="math inline">\(\mathcal{C}\)</span> has enough structure to do learning on it, so does <span class="math inline">\(\mathsf{CoKl}(\mathbb{R}^{n \times n} \times -)\)</span>.
Which in turn means that all the constructions for parametric bidirectional morphisms from our original paper apply here as well!
For instance, we can use the same kind of diagrams (drawn below) depicting a closed parametric lens whose parameters are being learned (from our <a href="https://arxiv.org/abs/2103.01931">original paper</a>) for the base category <span class="math inline">\(\mathsf{CoKl}(\mathbb{R}^{n \times n} \times -)\)</span>, and draw string diagrams of GCNN learning!</p>
<center>
<img src="../images/closed_supervised.png" alt="String diagram of a closed supervised learner." width="800" />
</center>
<p>I find this pretty exciting.</p>
<p>On the other hand, I see this paper merely as a stepping stone. It’s a neat idea - but there’s much more work to do. What’s making me deeply excited is where it could lead us further – to categorical models of full message passing Graph Neural Networks – and beyond.</p>
<p><br>
<br></p>
<p>Thanks to <a href="https://ievacepaite.com/">Ieva Čepaitė</a> for a read-through of this post.</p>

<div class="commentbox"></div>


<script>
commentBox('5769281040023552-proj');
</script>

        <div id="footer">
          <div class="inside">
            Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/"> CC BY-SA 4.0</a>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
            The theme originates in the <a href="http://katychuang.com/hakyll-cssgarden/gallery/">Hakyll-CSSGarden</a>.
          </div>
        </div>

          </div>
        </div>
    </body>
</html>
