<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PWZE1HYS87"></script>
        <script>
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
         gtag('js', new Date());

         gtag('config', 'G-PWZE1HYS87');
        </script>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="author" content="Bruno Gavranovic" />
        <meta name="viewport" content="width=device-width" />
        <meta http-equiv="Cache-Control" content="max-age=86400, must-revalidate" />
        <title>Meta-learning and Monads</title>
        <script src="../css/jquery.js"></script>
        <script src="../css/selectfile.js"></script>
        <link rel="stylesheet" type="text/css" title="hakyll_theme" href="../css/theprofessional.css" />
        <link href="https://fonts.googleapis.com/css?family=Titillium+Web" rel="stylesheet">

        <!-- MathJax font size adjustment -->
        <style>
            mjx-container {
                font-size: 80% !important;
            }

            /* Scale TikZ diagrams uniformly */
            .page svg {
                transform: scale(1.2) !important;
                transform-origin: center !important;
            }

            /* Improve code blocks */
            pre, code {
                background-color: #f8f9fa;
                border: 1px solid #e1e4e8;
                border-radius: 3px;
                font-family: 'Monaco', 'Menlo', 'Courier New', monospace;
                font-size: 0.9em;
            }

            pre {
                padding: 1em;
                overflow-x: auto;
            }

            code {
                padding: 0.2em 0.4em;
            }

            pre code {
                background: none;
                border: none;
                padding: 0;
            }
        </style>

        <!-- MathJax for regular math -->
        <script>
            MathJax = {
                tex: {
                    macros: {
                        coloneqq: '\\mathrel{\\vcenter{:}}=',
                        enskip: '\\hspace{0.5em}'
                    }
                }
            };
        </script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

        <!-- TikZJax for TikZ diagrams -->
        <link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
        <script src="https://tikzjax.com/v1/tikzjax.js"></script>
    </head>
    <body>
        <div class="highbar">&nbsp;</div>
        <div class="container-gallery">
        <div id="content" class="inside">

        <div id="header">
          <div class="box">
            <div id="logo" class="name">
                <h2><pageTitle><a href="../">Bruno Gavranović</a></pageTitle></h2>
            </div>
            <div id="navigation" class="pageslinks">
              <nav class="menuNav">
                <div class="menuItems">
                <a href="../" class="posts/2021-10-13-meta-learning-and-monads.md">Home</a>
                <a href="../archive.html" class="posts/2021-10-13-meta-learning-and-monads.md">Posts</a>
                <a href="../papers.html" class="posts/2021-10-13-meta-learning-and-monads.md">Papers</a>
                <a href="../research_programme.html" class="posts/2021-10-13-meta-learning-and-monads.md">Research Programme</a>
                <a href="../about.html" class="posts/2021-10-13-meta-learning-and-monads.md">About</a>
		<!-- <a href="/contact.html" class="posts/2021-10-13-meta-learning-and-monads.md">Contact</a> -->
                </div>
              </nav>
            </div>
        </div>
        </div>
            <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>

<div class="info">
    Posted on October 13, 2021
    
</div>

<h1 id="meta-learning-and-monads">Meta-learning and Monads</h1>
<p>Meta-learning is an exciting approach to machine learning. Instead of training models to do particular tasks, it trains models to <em>learn</em> how to do those tasks. Meta-learning is essentially learning squared: learning how to learn. There’s been plenty of exciting developments in this area, but in this blog post I want to focus on one interesting algebraic aspect of what it means to meta-learn.</p>
<p>Abstractly, a meta-learner is also a learner, one which has another learner nested inside of it. We might expect this meta-learner to satisfy a particular property: that these two levels of learners can be “collapsed” into just one big learner. Compare this with the concept of a monad. A monad <span class="math inline">\(T\)</span> on some category <span class="math inline">\(\mathcal C\)</span> has a property that for every object <span class="math inline">\(A : \mathcal C\)</span>, there is a map <span class="math inline">\(\mu_A : T(T(A)) \to T(A)\)</span> called “join”. This map joins, or collapses down two levels of context to just one. For instance, if you’ve got the list monad, that means that you can collapse a list of lists down to just a list. In Haskell there is done with the map</p>
<center>
<code>concat :: [[a]] -&gt; [a]</code>
</center>
<p>which concatenates lists. It takes, say, <code>[[1, 2],[4,5,6], [-37]]</code> and produces <code>[1,2,4,5,6,-37]</code>. Or, if you’ve got the probability monad, then you can take a probability distribution on <em>the space of probability distributions on some space <span class="math inline">\(A\)</span></em> and think of it just like a probability distribution on <span class="math inline">\(A\)</span>. That is, you collapse down two levels of probability distributions to one. Another example is the state monad. The join operation of the state monad tells us that two layers of stateful computation can be turned into just one layer of big stateful computation. And so on.</p>
<p>Thus a natural question to ask is: could meta-learning be formally interpreted as a monad? This is a deep question. Categorically formalising what “learning” is, even in the simplest case, is a daunting task.
We would need a base category, an endofunctor on it which describes “learning”, and two operations: join and unit (I haven’t discussed the unit yet, but I will).</p>
<p>In our recent paper <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a> we’ve got the construction Para which I believe is a first step.
Given any symmetric monoidal category <span class="math inline">\(\mathcal C\)</span> (throughout the post I’ll be thinking of the monoidal structure as Cartesian), we can form a category<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math inline">\(\mathbf{Para}(\mathcal C)\)</span> which is defined as follows. It has the same objects as <span class="math inline">\(\mathcal C\)</span>, but every morphism <span class="math inline">\(A \to B\)</span> in <span class="math inline">\(\mathbf{Para}(\mathcal C)\)</span> isn’t just a morphism in <span class="math inline">\(\mathcal C\)</span>. It’s a choice of some parameterising object <span class="math inline">\(P : \mathcal C\)</span> and a map <span class="math inline">\(f : P \otimes A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span>. We think of <span class="math inline">\(A\)</span> as the inputs to a neural network, <span class="math inline">\(B\)</span> as the outputs and <span class="math inline">\(P\)</span> as the parameter space of the neural network, whose elements are usually called <em>weights</em>. Visualised below, the information flows from <span class="math inline">\(A\)</span> on the left to <span class="math inline">\(B\)</span> on the right, while being transformed by <span class="math inline">\(P\)</span> coming from above. I think of the parameters as the ship’s wheel, controlling the rudder down below and steering the flow of water from the inputs to the outputs.
This means that for different choices of <span class="math inline">\(p : P\)</span>, via <span class="math inline">\(f(p, -)\)</span> we get different implementations of a map of type <span class="math inline">\(A \to B\)</span>. Abstractly, we can think of <span class="math inline">\((P, f) : \mathbf{Para}(\mathcal C)(A, B)\)</span> as a learner which is learning a map of type <span class="math inline">\(A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span>. Our learning algorithm will search through the parameter space <span class="math inline">\(P\)</span> in order to find a <span class="math inline">\(p : P\)</span> such that the map <span class="math inline">\(f(p, -) : A \to B\)</span> is <em>best</em>, according to some criteria.</p>
<center>
<img src="../images/parameterised_map.png" alt="A parameterised map" width="600" />
</center>
<p>Parameterised maps can be composed. If you’ve got a <span class="math inline">\(P\)</span>-parameterised morphism <span class="math inline">\(A \to B\)</span> and <span class="math inline">\(Q\)</span>-parameterised <span class="math inline">\(B \to C\)</span>, you can plug them together, obtaining a <span class="math inline">\(P \otimes Q\)</span>-parameterised morphism <span class="math inline">\(A \to C\)</span>. This is visualised below.</p>
<center>
<img src="../images/para_comp2.gif" alt="Composition of parameterized maps" width="600" />
</center>
<p>The category <span class="math inline">\(\mathbf{Para}(\mathcal C)\)</span> is useful because it allows us to state the fact that the parameter space of a neural network is more than just an object of the category: it’s part of the data of the morphism itself. There’s <a href="https://matteocapucci.files.wordpress.com/2021/09/main-annotated.pdf">a lot more to say about this category</a>, but what’s most interesting is that <span class="math inline">\(\mathbf{Para}(\mathcal C)\)</span> is itself a symmetric monoidal category. This allows us to apply <span class="math inline">\(\mathbf{Para}\)</span> to it again, yielding a category<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> with doubly-parameterised maps.</p>
<p>How do we think about these doubly-parameterised morphisms?
Just as the path we took from thinking about maps in <span class="math inline">\(\mathcal C\)</span> to maps in <span class="math inline">\(\mathbf{Para}(\mathcal C)\)</span> involved using an extra axis for the parameters, the same thing applies when we go from <span class="math inline">\(\mathbf{Para}(\mathcal C)\)</span> to <span class="math inline">\(\mathbf{Para}({\mathbf{Para}(\mathcal C)})\)</span> (I’ll write <span class="math inline">\(\mathbf{Para}^2(\mathcal C)\)</span> for double Para from now on). A map <span class="math inline">\(A \to B\)</span> in this category is a choice of some parameter object <span class="math inline">\(Q : \mathbf{Para}(\mathcal C)\)</span> (which is just an object <span class="math inline">\(Q\)</span> in <span class="math inline">\(\mathcal C\)</span>) and a morphism in <span class="math inline">\(\mathbf{Para}(\mathcal C)(Q \otimes A, B)\)</span>. But a morphism in <span class="math inline">\(\mathbf{Para}(\mathcal C)(Q \otimes A, B)\)</span> is itself a parameterised map! It involves a choice of an object <span class="math inline">\(P : \mathcal C\)</span> and a map <span class="math inline">\(f : P \otimes Q \otimes A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span>. So we see that there were two levels of parameters to unpack.
This can be neatly visualised below in three dimensions.</p>
<center>
<img src="../images/para_squared2.png" alt="Visualisation of maps in C, Para(C) and Para^2(C)" width="600" />
</center>
<p>In fact, <span class="math inline">\(\mathbf{Para}\)</span> itself is an endofunctor, it’s type is <span class="math inline">\(\mathbf{SMCCat} \to \mathbf{SMCCat}\)</span>, where the domain and codomain is the category of symmetric monoidal categories and symmetric monoidal functors.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> This means that <span class="math inline">\(\mathbf{Para}\)</span> can be applied not only to a symmetric monoidal category <span class="math inline">\(\mathcal C\)</span>, but also to a functor <span class="math inline">\(F: \mathcal C \to \mathcal D\)</span> between symmetric monoidal categories.</p>
<p>Finally, this brings us to the main idea of this blog post: <span class="math inline">\(\mathbf{Para}\)</span> is an endofunctor, but also more than that: it has the structure of a monad.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> The join of this monad tells us that for every object <span class="math inline">\(\mathcal C\)</span> in our category (and this object is now itself a symmetric monoidal category), there is a morphism <span class="math inline">\(\mu_A : \mathbf{Para}^2(\mathcal{C}) \to \mathbf{Para}(\mathcal C)\)</span> (and this morphism is a symmetric monoidal functor) which takes any doubly-parameterised category and reinterprets it as a (singly-)parameterised category. The previously described morphism <span class="math inline">\((Q, (P, f)) : \mathbf{Para}^2(\mathcal{C})\)</span> gets mapped to <span class="math inline">\((Q \otimes P, f) : \mathbf{Para}(\mathcal{C})\)</span>. Abstractly, it tells us that a <span class="math inline">\(Q\)</span>-parameterised learner of a <span class="math inline">\(P\)</span>-parameterised learner of a map <span class="math inline">\(A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span> can be reinterpreted of as a <span class="math inline">\(Q \otimes P\)</span>-parameterised learner of a map in <span class="math inline">\(A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span>.</p>
<p>What about the unit of the monad? Let’s think about what should happen. The unit map of an arbitrary monad <span class="math inline">\(T\)</span> on <span class="math inline">\(\mathcal C\)</span> assigns to each object <span class="math inline">\(A : \mathcal C\)</span> a map <span class="math inline">\(\eta_A : A \to T(X)\)</span>. We think of it as taking something outside of any context and putting it in a trivial context. For the list monad, it takes any element <code>a</code> and puts it in a list containing just that one element <code>a</code>. In Haskell, this is done with the map <code>return : a -&gt; [a]</code> which takes, say, an integer <code>3</code> and maps it to the list <code>[3]</code>. Similar story happens with the probability monad. Any element <span class="math inline">\(a : A\)</span> can be interpreted as a “deterministic” probability distribution <span class="math inline">\(\delta_A\)</span> using the dirac delta at <span class="math inline">\(a\)</span>. For the state monad, the unit takes an element <span class="math inline">\(a : A\)</span> and wraps in on a context which independently of state always returns <span class="math inline">\(a\)</span> (and leaves the state unchanged).</p>
<p>What would we expect to happen for learners? Well, since <span class="math inline">\(\mathbf{Para}(\mathcal{C})\)</span> somehow reinterprets the usually “unparameterised” category <span class="math inline">\(\mathcal{C}\)</span> as parameterised, then we expect the unit to interpret an unparameterised morphism <span class="math inline">\(f : A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span> as a parameterised one in <span class="math inline">\(\mathbf{Para}(\mathcal C)(A, B)\)</span>. Which parametrised morphism is it? We first need to pick a parameter space. And the only object we can even refer to in an arbitrary monoidal category <span class="math inline">\(\mathcal C\)</span> is the monoidal unit <span class="math inline">\(I\)</span>. Now we need to pick a morphism of type <span class="math inline">\(I \otimes A \to B\)</span>, and the natural choice is <span class="math inline">\(f \circ \lambda_A : I \otimes A \to B\)</span>, where <span class="math inline">\(\lambda_A : I \otimes A \to A\)</span> is the laxator of the monoidal category.</p>
<p>We didn’t do much, we just wrapped our morphism in some trivial context. This makes sense! If you look at the previous image, you can imagine that for the map <span class="math inline">\(A \to B\)</span> in <span class="math inline">\(\mathcal C\)</span> there’s a “secret”, but trivial input <span class="math inline">\(I\)</span> coming from the top. I’ve also drawn this explicitly below. This agrees with the idea of <em>not</em> drawing the wires of the monoidal unit.</p>
<center>
<img src="../images/para_unit2.png" alt width="600" />
</center>
<p>So the unit of this monad tells us that any computation can be thought of as a trivially parameterised computation. In more abstract terms: for a system that doesn’t have some internal parameters and <em>isn’t</em> learning, we can say that it’s <em>trivially learning</em>. We can think of it as having a parameter space with only one element in it: you can always only pick that one parameter and there’s nothing to learn. And this is it: it’s easy to check that unit and join, as defined for <span class="math inline">\(\mathbf{Para}\)</span>, satisfy the monad laws.</p>
<p>Now, there’s still a big unanswered question: what part of this actually describes learning? We’ve only described what it means to be parameterised. This is true – <span class="math inline">\(\mathbf{Para}\)</span> is only a first step. However, this is also where we use the power of compositionality of category theory. By substituting the base monoidal category <span class="math inline">\(\mathcal C\)</span> for the right category, we get very different things. As I’ve described in <a href="https://www.brunogavranovic.com/posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.html">my other post</a>, the category of learners is obtained when we set the base category to <span class="math inline">\(\mathbf{Optic}(\mathcal C)\)</span>, the category of what I call “bidirectional maps”. These are maps which compute a forward value, but also receive a “gradient” which they propagate backwards.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> The interaction between <span class="math inline">\(\mathbf{Para}\)</span> and <span class="math inline">\(\mathbf{Optic}\)</span> ends up propagating the right information to the right ports, which is quite astonishing.</p>
<p>But the story doesn’t end on a satisfying note. While optics allow us to talk about bidirectionality and parameter update, we need more. Learning is essentially about iteration: you do something, get feedback, update yourself and then go out into the world to try again. While <a href="https://www.ioc.ee/~mroman/data/notes/iterative-processes.pdf">some interesting things</a> have been written about iteration of learning, no unifying categorical perspective has been given yet.</p>
<p>In more practical terms, this missing structure needs to account for two kinds of learning: standard learning where a learner is iterated, seeing different data-points as input at each time step, and meta-learning, where the meta-learner now sees as input whole iterations of learning at each time step. There is a lot of details here, and a number of different kinds of meta-learning can be found in the literature. The paper <a href="https://arxiv.org/abs/1606.04474">Learning to learn by gradient descent by gradient descent</a> seems to be the closest to what I’m describing here.</p>
<p>There are many things left to understand. It’s hard to wrap our heads around what ought to happen when you’re learning to learn. We’re building a house, and it’s hard to do it without good foundations. Category theory provides a lot of steel beams, but it’s not clear how to use them just yet. Hopefully we can find a way and build skyscrapers people couldn’t have dreamt of a few hundred years ago.</p>
<p><br>
<br></p>
<p>Thanks to <a href="https://ievacepaite.com/">Ieva Čepaitė</a> and <a href="https://matteocapucci.wordpress.com/">Matteo Capucci</a> for providing feedback on a draft of this post.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This construction was originally defined in <a href="https://arxiv.org/abs/1711.10455">Backprop as Functor</a> in a specialised form. It is also technically a bicategory, and some care is needed to think of it as a category. More on <a href="https://ncatlab.org/nlab/show/para+construction">nLab</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Technically now a <em>tricategory</em>, but we’ll forget this detail for now.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>There’s some technical details with strictness that I’m omitting.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>As far as I know, this was first worked out by <a href="http://brendanfong.com/">Brendan Fong</a>, <a href="http://math.mit.edu/~dspivak/">David Spivak</a> and <a href="https://julesh.com/">Jules Hedges</a> back in 2018. I think I’m the first one who noticed a connection with meta-learning in <a href="https://twitter.com/bgavran3/status/1307260999209234444">this tweet</a> in September 2020. This tweet is coincidentally what this blog post started as.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>What I described are optics for the <em>multiplicative</em> monoidal action. We can get more interesting types of interaction by choosing different monoidal actions.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

<div class="commentbox"></div>


<script>
commentBox('5769281040023552-proj');
</script>

        <div id="footer">
          <div class="inside">
            Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/"> CC BY-SA 4.0</a>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
            The theme originates in the <a href="http://katychuang.com/hakyll-cssgarden/gallery/">Hakyll-CSSGarden</a>.
          </div>
        </div>

          </div>
        </div>
    </body>
</html>
