<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PWZE1HYS87"></script>
        <script>
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
         gtag('js', new Date());

         gtag('config', 'G-PWZE1HYS87');
        </script>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="author" content="Bruno Gavranovic" />
        <meta name="viewport" content="width=device-width" />
        <meta http-equiv="Cache-Control" content="max-age=86400, must-revalidate" />
        <title>Towards Categorical Foundations of Learning</title>
        <script src="../css/jquery.js"></script>
        <script src="../css/selectfile.js"></script>
        <link rel="stylesheet" type="text/css" title="hakyll_theme" href="../css/theprofessional.css" />
        <link href="https://fonts.googleapis.com/css?family=Titillium+Web" rel="stylesheet">

        <!-- MathJax font size adjustment -->
        <style>
            mjx-container {
                font-size: 80% !important;
            }

            /* Scale TikZ diagrams uniformly */
            .page svg {
                transform: scale(1.2) !important;
                transform-origin: center !important;
            }

            /* Improve code blocks */
            pre, code {
                background-color: #f8f9fa;
                border: 1px solid #e1e4e8;
                border-radius: 3px;
                font-family: 'Monaco', 'Menlo', 'Courier New', monospace;
                font-size: 0.9em;
            }

            pre {
                padding: 1em;
                overflow-x: auto;
            }

            code {
                padding: 0.2em 0.4em;
            }

            pre code {
                background: none;
                border: none;
                padding: 0;
            }
        </style>

        <!-- MathJax for regular math -->
        <script>
            MathJax = {
                tex: {
                    macros: {
                        coloneqq: '\\mathrel{\\vcenter{:}}=',
                        enskip: '\\hspace{0.5em}'
                    }
                }
            };
        </script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

        <!-- TikZJax for TikZ diagrams -->
        <link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
        <script src="https://tikzjax.com/v1/tikzjax.js"></script>
    </head>
    <body>
        <div class="highbar">&nbsp;</div>
        <div class="container-gallery">
        <div id="content" class="inside">

        <div id="header">
          <div class="box">
            <div id="logo" class="name">
                <h2><pageTitle><a href="../">Bruno Gavranović</a></pageTitle></h2>
            </div>
            <div id="navigation" class="pageslinks">
              <nav class="menuNav">
                <div class="menuItems">
                <a href="../" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Home</a>
                <a href="../archive.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Posts</a>
                <a href="../papers.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Papers</a>
                <a href="../research_programme.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Research Programme</a>
                <a href="../about.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">About</a>
		<!-- <a href="/contact.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Contact</a> -->
                </div>
              </nav>
            </div>
        </div>
        </div>
            <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>

<div class="info">
    Posted on March  3, 2021
    
</div>

<h1 id="towards-categorical-foundations-of-learning">Towards Categorical Foundations of Learning</h1>
<p>Having posted our paper on <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a> on arXiv, I decided to write a short update on the progress we made, and the staggering amount of potential I believe Category Theory has in the field of Deep Learning.</p>
<p>Deep Learning is notoriously an <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">ad-hoc field</a>. Despite its tremendous success, we lack a unifying perspective for this growing body of work. We have entire paradigms of how to effectively learn, but it’s still hard to precisely state what a neural network is and cover all the use cases. This is the contribution of our paper. We’re making a <em>step forward</em> in that regard by creating a foundation of neural networks terms of three things: 1) Parameterized maps, 2) Bidirectional data structures (Lenses/Optics) and 3) Reverse derivative categories.</p>
<p>Since our work is based on category theory, you might wonder the aforementioned concepts are, what <a href="https://arxiv.org/abs/1809.05923">Category theory even is</a>, or even why you would want to <a href="https://twitter.com/bgavran3/status/1335560269511204866">abstract away some details in neural networks</a>?
This is a question that deserves a proper answer. For now I’ll just say that our paper really answers the following question in a very precise way: “What is the minimal structure, in some suitable sense, that you need to have to perform learning?”. This is certainly valuable. Why? If you try answering that question you might discover, just as we did, that this structure ends up encapsulated some strange types of learning, with hints to even meta-learning. For instance, after defining our framework on neural networks on Euclidean spaces we realized that it includes learning not just in Euclidean spaces, but also on <a href="https://arxiv.org/abs/2101.10488">Boolean circuits</a>. This is pretty strange, how can you “differentiate” a Boolean circuit? It turns out you can, and this falls under the same framework of <a href="https://arxiv.org/abs/1910.07065">Reverse derivative categories</a>.</p>
<p>Another thing we discovered is that all the optimizers (standard gradient descent, momentum, Nesterov momentum, Adagrad, Adam etc.) are the same kind of structure neural networks themselves are - giving us hints that optimizers are in some sense “hardwired meta-learners”, just as <a href="https://arxiv.org/abs/1606.04474">Learning to Learn by Gradient Descent by Gradient Descent</a> describes.</p>
<p>Of course, I still didn’t tell you what this framework is, nor did I tell you how we defined neural networks. I’ll do that briefly now.
One of the main insights from our paper is that entirety of neural network learning can be framed in the language of <a href="https://arxiv.org/abs/1606.04474"><em>optics</em></a>, a bidirectional data structure used to encapsulate the essence of the forward-backward interaction (very closely related to <a href="https://julesh.com/2018/08/16/lenses-for-philosophers/">lenses</a>). This forward-backward interaction is animated below and summarized <a href="https://twitter.com/bgavran3/status/1364644337968103428">here</a>.</p>
<center>
<img src="../images/optics.gif" alt="Optics" width="600" />
</center>
<p>Optics tell us that data is propagated forward, some intermediate state is saved, and then data is propagated backward - and nothing else<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This is actually great, because we can then instantiate optics in a variety of settings. From propagating back gradients, to interesting things like <a href="https://arxiv.org/abs/1910.03656">propagating back utilities in game theory</a>, to using optics for <a href="https://blog.statebox.org/on-lawful-lenses-6e18a1e17bdf">lawful</a> updates - encapsulating the previously mentioned optimizers. But the general idea of optics is - just pick a base space/category/setting <span class="math inline">\(\mathcal{C}\)</span> in which you can compose processes in sequence and in parallel - and then you can form the space/category <span class="math inline">\(\mathbf{Optic}(\mathcal{C})\)</span> of maps which are bidirectional.</p>
<p>Optics themselves can too be put in sequence and in parallel, in completely natural ways you’d expect. Below you can see an animation of sequential optic composition, summarized <a href="https://twitter.com/bgavran3/status/1366202140788731908">here</a>.</p>
<center>
<img src="../images/optic_comp.gif" alt="Optic composition" width="600" />
</center>
<p>But optics by themselves aren’t enough. To talk about neural networks - or reinforcement learning agents - we need to allow optics to read from and write to some state that’s not accessible to the outside world. This is where the <span class="math inline">\(\mathbf{Para}\)</span> construction comes in.
Given any base space/category/setting <span class="math inline">\(\mathcal{C}\)</span> in which you can put compose processes in sequence and in parallel, you can form the “parameterized” space <span class="math inline">\(\mathbf{Para}(\mathcal{C})\)</span> of maps which have a “hidden” input. You can think of this as the private knowledge of the agent, or as the agent’s internal space.
These parameterized maps have all the nice setting you’d want - including parallel and sequential composition, as shown below.</p>
<center>
<img src="../images/para_comp.gif" alt="Composition of parameterized maps" width="600" />
</center>
<p>This is how we come to the main idea of our paper. <span class="math inline">\(\mathbf{Para}\)</span> by itself is not that useful, but when we apply it to <span class="math inline">\(\mathbf{Optic}(\mathcal{C})\)</span>, we get interesting things out.
This gives us the the important notion of <span class="math inline">\(\mathbf{Para(Optic(\mathcal{C}))}\)</span>: the category of parameterised optics. It’s a pretty complex beast, but maps inside are in some sense the “true shape” of neural networks. A parameterised optic is here a thing with 6 ports in total, organized in (input,output) pairs on its left, right and “internal” (drawn on the top) side. It has a pretty complex flow of information, being bidirectional, but “in one additional dimension”.</p>
<p>The image below shows this information flow, which is also explained in more detail <a href="https://twitter.com/bgavran3/status/1404438688831004673">here</a>, and <a href="https://youtu.be/tKM8JdXJEII?t=1470">here</a>.</p>
<center>
<img src="../images/para_optic.gif" alt="A parameterized optic" width="600" />
</center>
<p>These parameterised optics can be plugged in in sequence, in parallel, and it even turns out <a href="https://arxiv.org/abs/2103.01189">they form something called a topos</a> - allowing us to talk about the “internal language” of a learner, a deeply exciting prospect. They model not just neural networks, but agents found in <a href="https://arxiv.org/abs/1603.04641">game theory</a> as well. And as mentioned before, these optimizers which update the parameters of these learners are exactly of the shape of these learners themselves - opening up interesting questions about meta-learning.</p>
<center>
<img src="../images/para_optic_comp.gif" alt="Composition of parameterized optics" width="600" />
</center>
<p>There are all these various things you can do with them - and this is where the power of this framework comes in. But I’ll describe these things in future posts.</p>
<p>This short blog post was a fast, bumpy tour of a lot of the things that we’ve been working on here at <a href="http://msp.cis.strath.ac.uk/">MSP</a>. For a slower and more nuanced description check out our paper, or some of the many interesting related papers I collect in this <a href="https://github.com/bgavran/Category_Theory_Machine_Learning">github repository</a>.</p>
<p>There is so much more to do in this setting. The deep learning community is exploring a number of exciting neural network paradigms and ideas. From Generative Adversarial Networks and Transformers, over Graph Neural Networks to very strange ideas such as World Models and Differentiable Neural Computers. Understanding their categorical essence is part of our future work and I urge anybody with overlapping interests to get in touch.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>While in this blog post I focus on <em>multiplicative</em> optics (which perform internal computation <em>and</em> interact with the environment), there are many others, allowing for description of complex interaction protocols. Prisms are an example of <em>additive optics</em> (which perform internal computation <em>or</em> interact with the environment) allowing for conditional control flow.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

<div class="commentbox"></div>


<script>
commentBox('5769281040023552-proj');
</script>

        <div id="footer">
          <div class="inside">
            Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/"> CC BY-SA 4.0</a>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
            The theme originates in the <a href="http://katychuang.com/hakyll-cssgarden/gallery/">Hakyll-CSSGarden</a>.
          </div>
        </div>

          </div>
        </div>
    </body>
</html>
