<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114795894-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114795894-3');
</script>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="author" content="@katychuang" />
        <meta name="viewport" content="width=device-width" />
        <meta http-equiv="Cache-Control" content="max-age=86400, must-revalidate" />
        <title>Bruno Gavranović</title>
        <script src="../css/jquery.js"></script>
        <script src="../css/selectfile.js"></script>
        <link rel="stylesheet" type="text/css" title="hakyll_theme" href="../css/theprofessional.css" />
        <link href="https://fonts.googleapis.com/css?family=Titillium+Web" rel="stylesheet">
    </head>
    <body>
        <div class="highbar">&nbsp;</div>
        <div class="container-gallery">
        <div id="content" class="inside">

        <div id="header">
          <div class="box">
            <div id="logo" class="name">
                <h2><pageTitle><a href="../">Bruno Gavranović</a></pageTitle></h2>
            </div>
            <div id="navigation" class="pageslinks">
              <nav class="menuNav">
                <div class="menuItems">
                <a href="../" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Home</a>
                <a href="../research_philosophy.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Research Philosophy</a>
                <a href="../about.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">About</a>
                <a href="../contact.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Contact</a>
                <a href="../archive.html" class="posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.md">Posts</a>
                </div>
              </nav>
            </div>
        </div>
        </div>
            <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>

<div class="info">
    Posted on March  3, 2021
    
</div>

<h1 id="towards-categorical-foundations-of-learning">Towards Categorical Foundations of Learning</h1>
<p>Having put up on Arxiv our paper on <a href="https://arxiv.org/abs/2103.01931">Categorical Foundations of Gradient-Based Learning</a>, I thought I’d write a short post updating everyone on the progress we made and the staggering amount of potential I believe Category Theory has in the field of Deep Learning.</p>
<p>Deep Learning is notoriously an <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">ad-hoc field</a>. We have networks that work, entire paradigms of how to effectively learn, but it’s still hard to precisely state what a neural network is such that you cover all the use cases. This is the contribution of our paper. We’re making a <em>step forward</em> in that regard by defining a (2-categorical) foundation of neural networks terms of three things: 1) Parameterized maps, 2) Bidirectional data structures (Lenses/Optics) and 3) Reverse derivative categories.</p>
<p>Since our work is based on Category theory, you might wonder the aforementioned concepts are, what <a href="https://arxiv.org/abs/1809.05923">Category theory even is</a>, or even why you would want to <a href="https://twitter.com/bgavran3/status/1335560269511204866">abstract away some details in neural networks</a>? This is a question that deserves a proper answer. For now I’ll just say that our paper really answers the following question in a very precise way: “What is the minimal structure, in some suitable sense, that you need to have to perform learning?”. This is certainly valuable. Why? If you try answering that question you might discover, just as we did, that this structure, ended up encapsulated some strange types of learning, with hints to even meta-learning. For instance, after defining our framework on neural networks on Euclidean spaces we realized that it includes learning not just in Euclidean spaces, but also on <a href="https://arxiv.org/abs/2101.10488">Boolean circuits</a>. This is pretty strange, how can you “differentiate” a Boolean circuit? It turns out you can, and this falls under the same framework of <a href="https://arxiv.org/abs/1910.07065">Reverse derivative categories</a>.</p>
<p>Another thing we discovered is that all the optimizers (standard gradient descent, momentum, Nesterov momentum, Adagrad, Adam etc.) are the same kind of structure neural networks themselves are - giving us hints that optimizers are in some sense “hardwired meta-learners”, just as <a href="https://arxiv.org/abs/1606.04474">Learning to Learn by Gradient Descent by Gradient Descent</a> describes.</p>
<center>
<img src="../images/IMG_0349.GIF" alt="Optics" width="600" />
</center>
<p>Of course, I did not tell you what this framework is, nor did I tell you how we defined neural networks yet. I’ll do that briefly now. One of the main insights from our paper is that entirety of neural network learning can be framed in the language of <a href="https://arxiv.org/abs/1606.04474"><em>optics</em></a>, a bidirectional data structure used to encapsulate the essence of the forward-backward interaction (very closely related to <a href="https://julesh.com/2018/08/16/lenses-for-philosophers/">lenses</a>). This forward-backward interaction is animated above and summarized <a href="https://twitter.com/bgavran3/status/1364644337968103428">here</a>.</p>
<p>Optics tell us that data is propagated forward, some intermediate state is saved, and then data is propagated backward - and nothing else. This is actually great, because we can then instantiate optics in a variety of settings. From propagating back gradients, to interesting things like <a href="https://arxiv.org/abs/1910.03656">propagating back utilities in game theory</a>, to using optics for <a href="https://blog.statebox.org/on-lawful-lenses-6e18a1e17bdf">lawful</a> updates - encapsulating the previously mentioned optimizers. But the general idea of optics is - just pick a base space/category/setting <img width="12" alt="\mathcal{C}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAZCAQAAACBIibWAAAA1UlEQVR4nL1UURHDIAyNBSxgAQtYwMIsYCEWsDALWKiFWsDCFtLCXQmBfe3lrnel76VpkleANQx4DrvhMTXCAZ8eYU1HKERHjsKCt062RD3A9fvAAtTojjKm4WRRUqXn4SwS/ZzTDT0odH3iJMFrLsBJrV7PXx/N8mdtCnnVC4mrF25PbEC91jlqL9Ke1mDX05S4xu9/F+CfBJudlwLto4OcT2TBbA6GvFHkctjbiHIrk1ZqZkEZXl4Nqqy263ZHFhnKmx9WFQi33VuU/eTrryVzoNbiL7ZkZyKHSiAGAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-1px;" /> in which you can compose processes in sequence and in parallel - and then you can form the space/category <img width="69" alt="\mathbf{Optic}(\mathcal{C})" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIoAAAAiCAQAAAAqwYlsAAAEFUlEQVR4nN1ZjZWkIAymBVqwBVqwBVuwBVugBVugBVqwBVuwBQ+QgECCojs7d/fNe/tmGX6SjyQkwNj/BMFkU39u+vMnC/Fnw74AwfSFrJz17tOFlpEt9/XjbDJLbGz3n40pM8Fn6RnNKptZ9xkEW0/K5rD6LEGbnQ3hl8lodgPWqDZHhDSscsdvbPkURBBYPBjNjco0nVb6xfyVXo89IUJfayU8oyqzC24G2/YGcyME3NH2MZDyxFakkQ5HZyReTkQPbg2Z9LjYiA5hMkJ7V3pDC0UKrLxVnICCVatHfxFmvjlrSd3HYiYpdViqand+L5cWiZEVcAizFeqR88yERJaSXN3JrL9mbT1JKjt2MTeufPmjx3hX3gwHrT8LTrgcN8qX27ui0q90uIXThmTNe2TJ9V2oD5Bi9x5zOYlsb0/Ibvui3gEK14UuD7X7kDfmb8eCqtmjYWA17oQROFDWDyLXIwZYkwyjtP9szLqHdN82d9DF5W37GgiFEXEfx9C2Fnbau5gBFqoy4a3zzKyEroaBHNQs/mzZ65G46CVDy+5onY3QfcgHQCzhFa6RgjkvJALK7GXv5s1jXo8qL5oznhU3hq2JlPMU0caibQg/34z0wzEUpHBvITGM6mIGicZA2Rz3NC7X3kRKLthehCpIx4aiH46+IGUugjpkSunaJSkr5Q4kZtyy2kg5i0opm/dsIwWyorNzWDdckqiii82AkW0lCU5uCGZ1UrBelLJAoLjodyAnZSosDQNm9kMRm65BkKIYVMQ1gD3diRUyU6uNFJX9jwMjRf4cKVNQmK5tutDnbMJXpMiLfgdyUvTfQEpUmDbYESXuM5bylpS29JIgJVY2dNxWaI+nMUUn9paTklOKAzs1ymzmjAHNX0hSupCr4OU72FKePlOkHLPRp0+Zhp1JgWsnLDWL8mHKTNm6ETbzwS8nsFPMA9yjdp9S7h1OylAoVZJynqnMUzSqHE8iGla1wOaV1cxMWp6uHTBAS+lC9LWBRMZwn9Sfj26YuwvqnPesJEVkxcIBZWaOe9qhtgRJXuootvigLj0uUhG4kFzCVTU330BFyh8PIZRXs0cvNfmp1V4TRnu0VU2MaLGUFH5duHwa3I6mMqyIOvHGV7re3I1cyGoIpzbDEELqGso4RYa8mObPpxcAhQQuHlRPCZOhNbeW9B5+Q15qZiSaWQ22ZMb6pbuNQjfLR2GEm1zVWx+QxgqRvKxQ87be8/bkrAMRJ45nmqMavzqe1eNrMxKfuTy6j7Wx+MtBXWi+wrdJGenj9Bamly8UKL5NCnNPXU/BX7xLVvB9UvpH70UH5KsnGxSaUefG74J+I6xDFEf8D2B0pxN85OP9eo/5gRPYzOlb2/hLmJsf6fTjZ71/CG2OwHG7/gNjeyGB6cWlKwAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-5px;" /> of maps which are bidirectional.</p>
<p>You can put optics in sequence and in parallel, in completely natural ways you’d expect. Below you can see an animation of sequential optic composition, summarized <a href="https://twitter.com/bgavran3/status/1366202140788731908">here</a>.</p>
<center>
<img src="../images/IMG_0352.GIF" alt="Optic composition" width="600" />
</center>
<p>But optics by themselves aren’t enough. To talk about neural networks - or reinforcement learning agents - we need to allow optics to read from and write to some state that’s not accessible to the outside world. This is where the <img width="41" alt="\mathbf{Para}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFIAAAAYCAQAAADD5mIaAAABnUlEQVR4nNWWjdGDIAyGswIruIIrsIIrsAIrZAVXcAVXYIWu0BX6AYZfRQF79b6Xu56N6ctThEQAJ4T1ZMwgYYTHtUEq+NCIEV8UU8CfxjTiHjLVAAvF8RGuRCVII7ee8udUmc4gJd15/5wq0xlkuPfwzqyDfPiB//uVFP4eo8hgC1coWqgjsULtRZs968On9DWLcq48GiHnrAgN+gi52jnARDU2LlFhetSt4K0/kVCg2qMJUvj/6tYBs4fP7IT5FEgxpR3AV9uhyaMIyf2Qdoqt+KTrtGWOWSQtUUi9aj3MqPMoQsZD2e4tkr1k/vVawOYHsYm+S+00NnoUIXtOMD+BZMVfXXt8AZLRlkB/tI4g73jcgmTaWPnSIXzjbIGs8bgBOdFJnP1ZPXvcdzy6IV0mHsRqIWs9uiGXg1IhGiFrPbohV0h7RwxUC1nr0Q3pekcoLcy/Fret5LWHl0gKgNnKaE9bSSNteUVbnuuridbGvERI8gzde+9W45EovInEY83TIjFt84oyuZ128ZG9597t2uNLMv29tp90efwBxbfnngquB+kAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> construction comes in. Given any base space/category/setting <img width="12" alt="\mathcal{C}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAZCAQAAACBIibWAAAA1UlEQVR4nL1UURHDIAyNBSxgAQtYwMIsYCEWsDALWKiFWsDCFtLCXQmBfe3lrnel76VpkleANQx4DrvhMTXCAZ8eYU1HKERHjsKCt062RD3A9fvAAtTojjKm4WRRUqXn4SwS/ZzTDT0odH3iJMFrLsBJrV7PXx/N8mdtCnnVC4mrF25PbEC91jlqL9Ke1mDX05S4xu9/F+CfBJudlwLto4OcT2TBbA6GvFHkctjbiHIrk1ZqZkEZXl4Nqqy263ZHFhnKmx9WFQi33VuU/eTrryVzoNbiL7ZkZyKHSiAGAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-1px;" /> in which you can put compose processes in sequence and in parallel, you can form the “parameterized” space <img width="62" alt="\mathbf{Para}(\mathcal{C})" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHwAAAAiCAQAAAA3k3WyAAADLklEQVR4nNVZa7nDIAzFAhawUAu1UAu1UAtYwMIs1AIWaqEWdnmXQsJj693Ww599KU1zSEgCI+RuGAjvmk/VfPpPtnwQA1mLNCgZzWCRbCay9A5XKvEhyKI++m0MZDtRikGVhZI8w5iiZwt54Eot8ePVmPbmZFKt5fdA1fcX5Bknu3rKzdiNrWeqa22DjIH4GUwpsvK+HXYltGsgMEVZRvE4AXYyJSlGLEZcw/sdW/X/hTYdirdBeVgkkjTUNQSybA4l4ot7svdZfBGE8moOTTslpO3cspkjsnDRY4z48ezzO52CkUYVwT3L2DoyZ0DHVkpxbcQ/H+zai3k+50DOGUF/+9loWftVj0skeCF/r0jJm5BICMow4nN45j/GTKY9CiBPPnn0BtzMFsosmXRSNR0aOtBFJl07awysxaFEXCQFjZmKaWs7U+spszJyUOImDXETbs8oHdV1eKtSmc3dfU3VBibI8AmY+Bx84v3Fk8CnrnU4m8idTJow890A69aRbi+O7mUcK+hQg3gf+7EYs20hO/vzmaw6B8qdlUnn5XRGu46U+FYKXAQCj5GDeDyk6dbnJJFQJYWXYgRkvqFYlKahU8ea5WMGbokaoAV0eC9zjwXirUdDSEceotNLNl5MnLrtwEP6g4i/oyMnzr9L/DgK6jI0h6a2h3iLjh8jPrkMLEKOLoX6Ozow4ulBpIZLiPuZHJC1Em/VkWdjnr0XY0Jy9yXEbUU+l525k3iPjrM9dkNAdVxfV+xI05pXh4B24is592DewB7irTryLpu5WXnvLQpbYMWP1L0ej1eQhquKPo/XdTAgrFcXLekW2PGDSLbIBvOpmOh0w02WxTC4tCRdWI3q1+TM0QeRxek8uvVcW4sOiy0zeQiWckOemjdloXuHlo+Q6IQUj9J1jb6x3qKZozHnESS5zlxbXYeFAGLGVwQ/djTZWei8cOlNse7n372yr+mYwJ2rewB/7K2Xtkf3oeYnsHUfSc6AL69ugBkvRU1YgNuam0C+caevz/m39LfGiDYmdXD87uUOwP5LqWHI6v3tIF4IWPrl//wugig0VTDW7jd+FH1BS+G88AfesrMUPoA/iQAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-5px;" /> of maps which have a “hidden” input. You can think of this as the private knowledge of the agent, or as the agent’s internal space. These parameterized maps have all the nice setting you’d want - including parallel and sequential composition, as shown below.</p>
<center>
<img src="../images/IMG_0344.GIF" alt="Composition of parameterized maps" width="600" />
</center>
<p>This is how we come to the main idea of our paper. We apply the <img width="41" alt="\mathbf{Para}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFIAAAAYCAQAAADD5mIaAAABnUlEQVR4nNWWjdGDIAyGswIruIIrsIIrsAIrZAVXcAVXYIWu0BX6AYZfRQF79b6Xu56N6ctThEQAJ4T1ZMwgYYTHtUEq+NCIEV8UU8CfxjTiHjLVAAvF8RGuRCVII7ee8udUmc4gJd15/5wq0xlkuPfwzqyDfPiB//uVFP4eo8hgC1coWqgjsULtRZs968On9DWLcq48GiHnrAgN+gi52jnARDU2LlFhetSt4K0/kVCg2qMJUvj/6tYBs4fP7IT5FEgxpR3AV9uhyaMIyf2Qdoqt+KTrtGWOWSQtUUi9aj3MqPMoQsZD2e4tkr1k/vVawOYHsYm+S+00NnoUIXtOMD+BZMVfXXt8AZLRlkB/tI4g73jcgmTaWPnSIXzjbIGs8bgBOdFJnP1ZPXvcdzy6IV0mHsRqIWs9uiGXg1IhGiFrPbohV0h7RwxUC1nr0Q3pekcoLcy/Fret5LWHl0gKgNnKaE9bSSNteUVbnuuridbGvERI8gzde+9W45EovInEY83TIjFt84oyuZ128ZG9597t2uNLMv29tp90efwBxbfnngquB+kAAAAASUVORK5CYII=" class="inline-math" style="margin:0; vertical-align:-1px;" /> construction to the base not just of any <img width="12" alt="\mathcal{C}" height="12" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAZCAQAAACBIibWAAAA1UlEQVR4nL1UURHDIAyNBSxgAQtYwMIsYCEWsDALWKiFWsDCFtLCXQmBfe3lrnel76VpkleANQx4DrvhMTXCAZ8eYU1HKERHjsKCt062RD3A9fvAAtTojjKm4WRRUqXn4SwS/ZzTDT0odH3iJMFrLsBJrV7PXx/N8mdtCnnVC4mrF25PbEC91jlqL9Ke1mDX05S4xu9/F+CfBJudlwLto4OcT2TBbA6GvFHkctjbiHIrk1ZqZkEZXl4Nqqy263ZHFhnKmx9WFQi33VuU/eTrryVzoNbiL7ZkZyKHSiAGAAAAAElFTkSuQmCC" class="inline-math" style="margin:0; vertical-align:-1px;" />, but that of <img width="69" alt="\mathbf{Optic}(\mathcal{C})" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIoAAAAiCAQAAAAqwYlsAAAEFUlEQVR4nN1ZjZWkIAymBVqwBVqwBVuwBVugBVugBVqwBVuwBQ+QgECCojs7d/fNe/tmGX6SjyQkwNj/BMFkU39u+vMnC/Fnw74AwfSFrJz17tOFlpEt9/XjbDJLbGz3n40pM8Fn6RnNKptZ9xkEW0/K5rD6LEGbnQ3hl8lodgPWqDZHhDSscsdvbPkURBBYPBjNjco0nVb6xfyVXo89IUJfayU8oyqzC24G2/YGcyME3NH2MZDyxFakkQ5HZyReTkQPbg2Z9LjYiA5hMkJ7V3pDC0UKrLxVnICCVatHfxFmvjlrSd3HYiYpdViqand+L5cWiZEVcAizFeqR88yERJaSXN3JrL9mbT1JKjt2MTeufPmjx3hX3gwHrT8LTrgcN8qX27ui0q90uIXThmTNe2TJ9V2oD5Bi9x5zOYlsb0/Ibvui3gEK14UuD7X7kDfmb8eCqtmjYWA17oQROFDWDyLXIwZYkwyjtP9szLqHdN82d9DF5W37GgiFEXEfx9C2Fnbau5gBFqoy4a3zzKyEroaBHNQs/mzZ65G46CVDy+5onY3QfcgHQCzhFa6RgjkvJALK7GXv5s1jXo8qL5oznhU3hq2JlPMU0caibQg/34z0wzEUpHBvITGM6mIGicZA2Rz3NC7X3kRKLthehCpIx4aiH46+IGUugjpkSunaJSkr5Q4kZtyy2kg5i0opm/dsIwWyorNzWDdckqiii82AkW0lCU5uCGZ1UrBelLJAoLjodyAnZSosDQNm9kMRm65BkKIYVMQ1gD3diRUyU6uNFJX9jwMjRf4cKVNQmK5tutDnbMJXpMiLfgdyUvTfQEpUmDbYESXuM5bylpS29JIgJVY2dNxWaI+nMUUn9paTklOKAzs1ymzmjAHNX0hSupCr4OU72FKePlOkHLPRp0+Zhp1JgWsnLDWL8mHKTNm6ETbzwS8nsFPMA9yjdp9S7h1OylAoVZJynqnMUzSqHE8iGla1wOaV1cxMWp6uHTBAS+lC9LWBRMZwn9Sfj26YuwvqnPesJEVkxcIBZWaOe9qhtgRJXuootvigLj0uUhG4kFzCVTU330BFyh8PIZRXs0cvNfmp1V4TRnu0VU2MaLGUFH5duHwa3I6mMqyIOvHGV7re3I1cyGoIpzbDEELqGso4RYa8mObPpxcAhQQuHlRPCZOhNbeW9B5+Q15qZiSaWQ22ZMb6pbuNQjfLR2GEm1zVWx+QxgqRvKxQ87be8/bkrAMRJ45nmqMavzqe1eNrMxKfuTy6j7Wx+MtBXWi+wrdJGenj9Bamly8UKL5NCnNPXU/BX7xLVvB9UvpH70UH5KsnGxSaUefG74J+I6xDFEf8D2B0pxN85OP9eo/5gRPYzOlb2/hLmJsf6fTjZ71/CG2OwHG7/gNjeyGB6cWlKwAAAABJRU5ErkJggg==" class="inline-math" style="margin:0; vertical-align:-5px;" />. This gives us the the important notion of <img width="119" alt="\mathbf{Para(Optic(\mathcal{C}))}" height="17" src="data:image/png;base64,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" class="inline-math" style="margin:0; vertical-align:-5px;" />: the category of open learners/generalized neural networks/reinforcement learning agents. It’s a pretty complex beast, but maps inside are in some sense the “true shape” of learners/agents. A learner is here a thing with 6 ports in total, organized in (input,output) pairs on its left, right and “internal” (drawn on the top) side. It has a pretty complex flow of information, being bidirectional, but “in one additional dimension”. To see what I mean, see <a href="https://youtu.be/tKM8JdXJEII?t=1470">this video (timestamped)</a>.</p>
<p>These learners can be plugged in in sequence, in parallel, and it even turns out <a href="https://arxiv.org/abs/2103.01189">they form something called a topos</a> - allowing us to talk about the “internal language” of a learner, a deeply exciting prospect. They model not just neural networks, but agents found in <a href="https://arxiv.org/abs/1603.04641">game theory</a> as well. And as mentioned before, these optimizers which update the parameters of these learners are exactly of the shape of these learners themselves - opening up interesting questions about meta-learning.</p>
<p>There are all these various things you can do with them - and this is where the power of this framework comes in. But these are some of the things I’ll describe in future blog posts.</p>
<p>This short blog post was a fast, bumpy tour of a lot of the things that we’ve been working on here at <a href="http://msp.cis.strath.ac.uk/">MSP</a>. For a slower and more nuanced description check out our paper, or some of the many interesting related papers I collect in this <a href="https://github.com/bgavran/Category_Theory_Machine_Learning">github repository</a>.</p>
<p>There is so much more to do in this setting. The deep learning community is exploring a number of exciting neural network paradigms and ideas. From Generative Adversarial Networks and Transformers, over Graph Neural Networks to very strange ideas such as World Models and Differentiable Neural Computers. Understanding their categorical essence is part of our future work and I urge anybody with overlapping interests to get in touch.</p>

<div class="commentbox"></div>


<script>
commentBox('5769281040023552-proj');
</script>

        <div id="footer">
          <div class="inside">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
            This theme was designed by <a href="http://twitter.com/katychuang">Dr. Kat</a> and showcased in the <a href="http://katychuang.com/hakyll-cssgarden/gallery/">Hakyll-CSSGarden</a>


          </div>
        </div>

          </div>
        </div>
    </body>
</html>
